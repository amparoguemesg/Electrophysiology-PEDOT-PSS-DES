{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bd5f0-c05b-4422-abdc-766d322ed30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "sys.path.append('../datasets')\n",
    "from load_intan_rhs_format.load_intan_rhs_format import read_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191e8f6-602c-4dbd-8ad9-f41cc302d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = \"../PEDOT-DES\"\n",
    "\n",
    "base_path = \"%s/%s/rhs_recordings\"%(experiments_path, 'rat2-RightNerve' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7063b1-0547-4727-8b21-078787379d84",
   "metadata": {},
   "source": [
    "### Loads all folders at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb733c-36b1-4c29-b842-fe3d6b04453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_ports_rhs(path,fileType='rhs', downsample=1,verbose=0):\n",
    "    \"\"\"\n",
    "    Load all .rhs files in a folder and return a dict with port-separated DataFrames and metadata.\n",
    "    \n",
    "    Returns:\n",
    "        df_by_port (dict): Port-wise DataFrames with channels as columns.\n",
    "        fs (float): Sampling frequency.\n",
    "        port_info (list): List of dicts with channel metadata.\n",
    "        full_df (DataFrame): Complete DataFrame with all channels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Gather all files\n",
    "    files = sorted(glob.glob(os.path.join(path, f'*.{fileType}'), recursive=True))\n",
    "    if verbose:\n",
    "        print(f\"Found {len(files)} {fileType} files in: {path}\")\n",
    "        print(files)\n",
    "    \n",
    "    amp_data = []\n",
    "    time_data = []\n",
    "    fs = None\n",
    "    all_port_info = None\n",
    "\n",
    "    # Loop through each file\n",
    "    for count, file in enumerate(files):\n",
    "        print(count)\n",
    "        print(f\"Loading file {count+1}/{len(files)}: {file}\")\n",
    "        data = read_data(file, verbose=verbose)\n",
    "        \n",
    "        # Sampling frequency (assume constant)\n",
    "        if fs is None:\n",
    "            fs = data['frequency_parameters']['amplifier_sample_rate']\n",
    "        \n",
    "        # Channel metadata (assume constant)\n",
    "        if all_port_info is None:\n",
    "            all_port_info = data['amplifier_channels']\n",
    "        \n",
    "        # Amplifier data\n",
    "        amp = data['amplifier_data'].T  # shape: [samples, channels]\n",
    "        amp = amp[::downsample]\n",
    "        amp_data.append(amp)\n",
    "\n",
    "        # Time vector\n",
    "        t = data['t'] if 't' in data else data['t_amplifier']\n",
    "        t = t[::downsample]\n",
    "        time_data.append(t)\n",
    "\n",
    "    # Concatenate across files\n",
    "    amp_data = np.vstack(amp_data)  # shape: [total_samples, channels]\n",
    "    time_data = np.concatenate(time_data)\n",
    "\n",
    "    # Create column labels from amplifier_channels\n",
    "    channel_names = [f\"{ch['port_name']}_{ch['native_channel_name']}\" for ch in all_port_info]\n",
    "    port_names = [ch['port_name'] for ch in all_port_info]\n",
    "\n",
    "    df_all = pd.DataFrame(amp_data, columns=channel_names)\n",
    "    df_all['time'] = time_data\n",
    "\n",
    "    # Split into port-wise DataFrames\n",
    "    print('split')\n",
    "    df_by_port = {}\n",
    "    for port in set(port_names):\n",
    "        cols = [f\"{ch['port_name']}_{ch['native_channel_name']}\" for ch in all_port_info if ch['port_name'] == port]\n",
    "        df_port = df_all[cols + ['time']].copy()\n",
    "        df_by_port[port] = df_port\n",
    "\n",
    "    return df_by_port, fs, all_port_info, df_all\n",
    "\n",
    "def is_valid_recording_folder(foldername):\n",
    "    \"\"\"Ignore calibration or malformed folders\"\"\"\n",
    "    invalid_keywords = ['calibration', 'test']\n",
    "    return not any(kw in foldername.lower() for kw in invalid_keywords)\n",
    "\n",
    "def parse_condition_name(foldername):\n",
    "    \"\"\"Extract a clean condition name for indexing.\"\"\"\n",
    "    parts = foldername.split('_')\n",
    "    try:\n",
    "        idx = next(i for i, p in enumerate(parts) if 'pulses' in p)\n",
    "        return foldername #'_'.join(parts[:idx+3])\n",
    "    except StopIteration:\n",
    "        return foldername  # fallback\n",
    "\n",
    "def save_impedance_info(port_info, save_path, rat_id):\n",
    "    \"\"\"Save impedance magnitude and phase info from amplifier channels.\"\"\"\n",
    "    impedance_data = []\n",
    "    for ch in port_info:\n",
    "        impedance_data.append({\n",
    "            'port_name': ch['port_name'],\n",
    "            'native_channel_name': ch['native_channel_name'],\n",
    "            'electrode_impedance_magnitude': ch['electrode_impedance_magnitude'],\n",
    "            'electrode_impedance_phase': ch['electrode_impedance_phase']\n",
    "        })\n",
    "\n",
    "    impedance_df = pd.DataFrame(impedance_data)\n",
    "    csv_path = f'{save_path}_impedance_summary.csv'\n",
    "    print(csv_path)\n",
    "    impedance_df.to_csv(csv_path, index=False)\n",
    "    print(f\"[✓] Saved impedance summary to {csv_path}\")\n",
    "    \n",
    "def load_and_save_data(base_path, rat_id='rat', downsample=1, force_reload=False, verbose=0):\n",
    "    \"\"\"\n",
    "    Load RHS recordings from all folders in the base path.\n",
    "    Saves each cuff recording in a separate dictionary and groups the stimulation recordings in one.\n",
    "    \"\"\"\n",
    "    os.makedirs(f'{base_path}/saved_pkls', exist_ok=True)\n",
    "    os.makedirs(f'{base_path}/saved_stim_pkls', exist_ok=True)\n",
    "    \n",
    "    folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "    folders = [f for f in folders if is_valid_recording_folder(f)]  # filter only valid folders\n",
    "    \n",
    "    cuff_data = {}  # Dictionary for cuff recordings\n",
    "    stim_data = {}  # Dictionary for grouped stimulation data\n",
    "    \n",
    "    # Separate the folders based on cuff and stimulation data\n",
    "    cuff_folders = [f for f in folders if not any(kw in f.lower() for kw in [\"saved\", \"calibration\", \"stim\"])]\n",
    "    print(cuff_folders)\n",
    "    stim_folder = [f for f in folders if \"stimulation\" in f]  # Stimulation folder (with \"stim\" in the name)\n",
    "    print(stim_folder)\n",
    "\n",
    "    # Process cuff recordings\n",
    "    for folder in cuff_folders:\n",
    "        print(f\"Loading cuff data from folder: {folder}\")\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        condition_name = parse_condition_name(folder)\n",
    "\n",
    "        # Define save path for pickle\n",
    "        save_path = f'{base_path}/saved_pkls/{condition_name}.pkl'\n",
    "        \n",
    "        # Load from pickle if exists and not forcing reload\n",
    "        if os.path.exists(save_path) and not force_reload:\n",
    "            print(f\"[✓] Loading saved cuff data for {rat_id} from {save_path}\")\n",
    "            with open(save_path, 'rb') as f:\n",
    "                data_dict, fs, port_info, full_df = pickle.load(f)\n",
    "            cuff_data[condition_name] = data_dict\n",
    "            continue  # Skip to the next folder if data is already saved\n",
    "\n",
    "        # Otherwise, load fresh data\n",
    "        data_dict = {}\n",
    "\n",
    "        # Load data from all ports (adjust this function to your setup)\n",
    "        df_by_port, fs, port_info, full_df = load_all_ports_rhs(path=folder_path, downsample=downsample)\n",
    "        \n",
    "        # Save impedance info for this cuff recording\n",
    "        save_impedance_info(port_info, f'{base_path}/saved_pkls/{condition_name}', rat_id)\n",
    "\n",
    "        # Add to dict\n",
    "        data_dict['Port A'] = df_by_port.get('Port A')\n",
    "        data_dict['Port B'] = df_by_port.get('Port B')\n",
    "        data_dict['Port C'] = df_by_port.get('Port C')\n",
    "\n",
    "        if verbose > 0:\n",
    "            for port in ['Port A', 'Port B', 'Port C']:\n",
    "                print(f\"Data for {port} in condition {condition_name}:\")\n",
    "                print(df_by_port.get(port, f\"No data for {port}\"))\n",
    "        \n",
    "        # Save the cuff data\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump((data_dict, fs, port_info, full_df), f)\n",
    "            print(f\"[✓] Saved cuff data for {rat_id} to {save_path}\")\n",
    "        \n",
    "        # Store the data in the cuff dictionary\n",
    "        cuff_data[condition_name] = data_dict\n",
    "\n",
    "    # Process stimulation folder\n",
    "    if stim_folder:\n",
    "        stim_folder_path = os.path.join(base_path, stim_folder[0])  # Assuming there's only one stim folder\n",
    "        print(f\"Loading stimulation data from folder: {stim_folder_path}\")\n",
    "        \n",
    "        # Create a grouped dictionary for the stimulation subfolders\n",
    "        grouped_stim_data = {}\n",
    "\n",
    "        stim_subfolders = [f for f in os.listdir(stim_folder_path) if os.path.isdir(os.path.join(stim_folder_path, f))]\n",
    "        \n",
    "        for subfolder in stim_subfolders:\n",
    "            subfolder_path = os.path.join(stim_folder_path, subfolder)\n",
    "            condition_name = parse_condition_name(subfolder)\n",
    "\n",
    "            # Define save path for pickle\n",
    "            stim_save_path = f'{base_path}/saved_stim_pkls/{condition_name}.pkl'\n",
    "\n",
    "            # Load from pickle if exists and not forcing reload\n",
    "            if os.path.exists(stim_save_path) and not force_reload:\n",
    "                print(f\"[✓] Loading saved stimulation data for {rat_id} from {stim_save_path}\")\n",
    "                with open(stim_save_path, 'rb') as f:\n",
    "                    stim_data_dict, fs, port_info, full_df = pickle.load(f)\n",
    "                grouped_stim_data[condition_name] = stim_data_dict\n",
    "                continue  # Skip if data is already saved\n",
    "\n",
    "            # Otherwise, load fresh data for each subfolder\n",
    "            stim_data_dict = {}\n",
    "\n",
    "            # Load data from all ports (adjust this function to your setup)\n",
    "            df_by_port, fs, port_info, full_df = load_all_ports_rhs(path=subfolder_path, downsample=downsample)\n",
    "\n",
    "            # Save impedance info for this cuff recording\n",
    "            save_impedance_info(port_info,f'{base_path}/saved_stim_pkls/{condition_name}', rat_id)\n",
    "\n",
    "            # Add to dict\n",
    "            stim_data_dict['Port A'] = df_by_port.get('Port A')\n",
    "            stim_data_dict['Port B'] = df_by_port.get('Port B')\n",
    "            stim_data_dict['Port C'] = df_by_port.get('Port C')\n",
    "\n",
    "            if verbose > 0:\n",
    "                for port in ['Port A', 'Port B', 'Port C']:\n",
    "                    print(f\"Data for {port} in stimulation condition {condition_name}:\")\n",
    "                    print(df_by_port.get(port, f\"No data for {port}\"))\n",
    "\n",
    "            # Save the stimulation data\n",
    "            with open(stim_save_path, 'wb') as f:\n",
    "                pickle.dump((stim_data_dict, fs, port_info, full_df), f)\n",
    "                print(f\"[✓] Saved stimulation data for {rat_id} to {stim_save_path}\")\n",
    "\n",
    "            # Store the stimulation data in the grouped dictionary\n",
    "            grouped_stim_data[condition_name] = stim_data_dict\n",
    "            \n",
    "        # Store grouped stimulation data\n",
    "        stim_data[stim_folder[0]] = grouped_stim_data\n",
    "\n",
    "    return cuff_data, stim_data, fs, port_info, full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f9732-e0dd-4633-a8a4-f94b5634fb00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#---------------------- Change rat_ID\n",
    "raw_data, stim_data, fs, port_info, full_df  = load_and_save_data(base_path, rat_id='Rat_02-RN', downsample=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecc9f7-c237-44d8-adc9-a6df1f03d334",
   "metadata": {},
   "source": [
    "#### Print Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522f8fe-7636-4ea7-ac1c-6a1f78d9c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_impedance_boxplot(port_info):\n",
    "    \"\"\"\n",
    "    Plot a boxplot of electrode impedance magnitudes grouped by port.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the list of channel dicts\n",
    "    imp_df = pd.DataFrame([{\n",
    "        'port': ch['port_name'],\n",
    "        'channel': ch['native_channel_name'],\n",
    "        'impedance_mag': ch['electrode_impedance_magnitude'],\n",
    "        'impedance_phase': ch['electrode_impedance_phase']\n",
    "    } for ch in port_info])\n",
    "\n",
    "    # Drop missing or zero values (optional)\n",
    "    imp_df = imp_df[imp_df['impedance_mag'] > 0]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(data=imp_df, x='port', y='impedance_mag', palette='Set2')\n",
    "    plt.title(\"Electrode Impedance Magnitudes by Port\")\n",
    "    plt.ylabel(\"Impedance Magnitude (Ohms)\")\n",
    "    plt.xlabel(\"Port\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    return imp_df  # Return for inspection if needed\n",
    "\n",
    "imp_df = plot_impedance_boxplot(port_info)\n",
    "print(imp_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8970ec-310c-4b65-91ee-29d421c15c00",
   "metadata": {},
   "source": [
    "## Recording analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214a2f1-1eff-4289-9c47-9ff94281c40d",
   "metadata": {},
   "source": [
    "Loads from saved_pkls folder the relevant pkl based on condtion, and then saved the outputs in a general folder for each condition in the PEDOT-DES folder, to be able then to just quickly extract per animal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3924a-fece-4fef-9700-624be8bff595",
   "metadata": {},
   "source": [
    "### Load pre-saved data - non-stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbacf9c8-cd0a-4380-beff-0453c88d161d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select rat\n",
    "rat_id = 'Rat_02-RN'\n",
    "\n",
    "base_path = \"%s/%s/rhs_recordings\"%(experiments_path, 'rat2-RightNerve' ) \n",
    "\n",
    "# Select file and condition - recording name per rat\n",
    "condition_name = 'baseline_250721_164614' # 'Rat_02-RN'\n",
    "\n",
    "condition = 'evoked' # \n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "save_path = f'{experiments_path}/{condition}' # Save all animals into same folder\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Load from pickle if exists \n",
    "if os.path.exists(f'{base_path}/saved_pkls'):\n",
    "    print(f\"[✓] Loading saved cuff data for {rat_id} from {f'{base_path}/saved_pkls'}\")\n",
    "    with open(f'{base_path}/saved_pkls/{condition_name}.pkl', 'rb') as f:\n",
    "        raw_data, fs, port_info, full_df = pickle.load(f)\n",
    "\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe1083e-62f0-4157-90f6-4e5c9bcdc75e",
   "metadata": {},
   "source": [
    "#### Plot pre-saved Impedances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c26b54-f757-428c-b76a-248c40cd5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "csv_path = f'{base_path}/saved_pkls/{condition_name}_impedance_summary.csv'  # Update as needed\n",
    "imp_df = pd.read_csv(csv_path)\n",
    "print(imp_df)\n",
    "\n",
    "# Remove Port C from plot\n",
    "imp_df = imp_df[imp_df['port_name'] != 'Port C']\n",
    "\n",
    "# Count A and B channels based on 'Port A_' and 'Port B_' in the 'Electrode' column\n",
    "num_A_channels = imp_df['port_name'].str.contains('Port A').sum()\n",
    "num_B_channels = imp_df['port_name'].str.contains('Port B').sum()\n",
    "\n",
    "# Create a boxplot of impedance magnitudes grouped by port\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='port_name', y='electrode_impedance_magnitude', data=imp_df, palette='Set2')\n",
    "\n",
    "# Annotate number of channels on top of each violin\n",
    "plt.text(x=0, y=imp_df['electrode_impedance_magnitude'].min() + 0.5, s=f\"n={num_A_channels}\", \n",
    "         ha='center', va='bottom', fontsize=20)\n",
    "plt.text(x=1, y=imp_df['electrode_impedance_magnitude'].min() -0.5, s=f\"n={num_B_channels}\", \n",
    "         ha='center', va='bottom', fontsize=20)\n",
    "\n",
    "# Plot aesthetics\n",
    "plt.title('Electrode Impedance Magnitude by Port')\n",
    "plt.xlabel('Port')\n",
    "plt.ylabel('Impedance Magnitude (Ohms)')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'{save_path}/{rat_id}_impedances.svg', dpi=300)\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846105d-eb7b-4163-b612-fcdb75d466b9",
   "metadata": {},
   "source": [
    "#### Remove high-impedance electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3f4b7-70fb-42d4-8259-340c110945e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hig-impedance electrodes\n",
    "# Step 1: Get the list of high-impedance channels from df_imp\n",
    "high_imp_rows = imp_df[imp_df['electrode_impedance_magnitude'] > 100000]\n",
    "columns_to_drop = [f\"{row['port_name']}_{row['native_channel_name']}\" for _, row in high_imp_rows.iterrows()]\n",
    "\n",
    "# Step 2: Drop those columns from each DataFrame in raw_data\n",
    "for key in raw_data:\n",
    "    raw_data[key] = raw_data[key].drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Optional: print confirmation\n",
    "print(f\"Dropped {len(columns_to_drop)} channels from each DataFrame in raw_data.\")\n",
    "#print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a162867-cb78-4544-88fb-fbdf8a8b1cf9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Plot raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48386fc1-35ec-4ce6-beaa-4fdee091ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data_superimposed(df_port_a, df_port_b, title):\n",
    "    # Check if the DataFrames are empty\n",
    "    if df_port_a is None or df_port_a.empty or df_port_b is None or df_port_b.empty:\n",
    "        print(\"No data found for the specified ports.\")\n",
    "        return\n",
    "\n",
    "    # Prepare the figure with two subplots, sharing the X-axis\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True, sharey=True)\n",
    "\n",
    "    # Plot Port A data\n",
    "    if 'time' in df_port_a.columns:\n",
    "        time_a = df_port_a['time']\n",
    "        df_port_a = df_port_a.drop(columns=['time'])  # Remove 'time' column for plotting\n",
    "    else:\n",
    "        time_a = range(len(df_port_a))  # If no time column, use index as time\n",
    "\n",
    "    for electrode in df_port_a.columns:\n",
    "        ax1.plot(time_a, df_port_a[electrode], label=electrode)\n",
    "    \n",
    "    ax1.set_title(\"Port A - %s\" %title)\n",
    "    ax1.set_ylabel('Signal Amplitude')\n",
    "    ax1.set_ylim([-100, 100])\n",
    "    ax1.legend(loc='best', title='Electrodes')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot Port B data\n",
    "    if 'time' in df_port_b.columns:\n",
    "        time_b = df_port_b['time']\n",
    "        df_port_b = df_port_b.drop(columns=['time'])  # Remove 'time' column for plotting\n",
    "    else:\n",
    "        time_b = range(len(df_port_b))  # If no time column, use index as time\n",
    "\n",
    "    for electrode in df_port_b.columns:\n",
    "        ax2.plot(time_b, df_port_b[electrode], label=electrode)\n",
    "    \n",
    "    ax2.set_title(\"Port B - %s\" %title)\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('Signal Amplitude')\n",
    "    ax2.legend(loc='best', title='Electrodes')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5933a4fe-3004-4573-9464-466e18f15465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot raw\n",
    "plot_data_superimposed(raw_data['Port A'].iloc[0:1*30000], raw_data['Port B'].iloc[0:1*30000], title='Raw data')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f'{save_path}/{rat_id}_1sec_raw.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfce58-f6c2-4df8-b9ea-fcdcd6d3b2ab",
   "metadata": {},
   "source": [
    "#### Filter and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e2664-fe29-4c44-a362-18b90f4f27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(signal, fs, lowcut=200.0, highcut=3000.0, order=4):\n",
    "    \"\"\"\n",
    "    Band-pass filter the input signal between lowcut and highcut frequencies.\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    \n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def apply_bandpass_to_rat_signal(raw_data, fs, lowcut=200.0, highcut=3000.0, order=4):\n",
    "    \"\"\"\n",
    "    Apply bandpass filter to all electrodes in Port A and Port B of raw_data.\n",
    "    Returns a new dictionary with filtered signals.\n",
    "    \"\"\"\n",
    "    filtered_signal = {}\n",
    "\n",
    "    for port in ['Port A', 'Port B']:\n",
    "        df = raw_data[port].copy()\n",
    "        if 'time' in df.columns:\n",
    "            time = df['time']\n",
    "            df = df.drop(columns=['time'])\n",
    "        else:\n",
    "            time = np.arange(len(df)) / fs\n",
    "\n",
    "        df_filtered = pd.DataFrame(index=df.index)\n",
    "        for col in df.columns:\n",
    "            df_filtered[col] = bandpass_filter(df[col].values, fs, lowcut, highcut, order)\n",
    "        \n",
    "        df_filtered.insert(0, 'time', time)\n",
    "        filtered_signal[port] = df_filtered\n",
    "\n",
    "    return filtered_signal\n",
    "\n",
    "#------------------------------------------------------\n",
    "# Set your sampling frequency\n",
    "fs = 30000  # e.g. 30 kHz\n",
    "\n",
    "# Apply bandpass filter to all electrodes\n",
    "filtered_signal = apply_bandpass_to_rat_signal(raw_data, fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c5bc3-ef55-4bf1-9716-d3b98e8554d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one electrode column from each port (e.g. the first non-time column)\n",
    "ch_a = filtered_signal['Port A'].columns[1]  # Skip 'time'\n",
    "ch_b = filtered_signal['Port B'].columns[1]\n",
    "\n",
    "# Extract data\n",
    "df_a = filtered_signal['Port A'][['time', ch_a]]#.iloc[0*60*fs:4*60*fs]\n",
    "df_b = filtered_signal['Port B'][['time', ch_b]]#.iloc[0*60*fs:4*60*fs]\n",
    "\n",
    "# Plot\n",
    "plot_data_superimposed(df_a, df_b, title='Filtered data: Single channel')\n",
    "\n",
    "plt.savefig(f'{save_path}/{rat_id}_singleCh_filtered.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235c577-5637-419f-bb51-fd3d82493edb",
   "metadata": {},
   "source": [
    "#### Spike detection and spike metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780417be-fe82-4f0b-8457-2013fdb30aa4",
   "metadata": {},
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c245686-37de-4618-b89d-db57ae740adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_negative_spikes(signal, fs, threshold_factor=5, window_ms=2):\n",
    "    \"\"\"\n",
    "    Detect negative spikes and extract waveforms.\n",
    "    \"\"\"\n",
    "    window_size = int(window_ms * fs / 1000)  # ±window_size around spike\n",
    "    #std = np.std(signal)\n",
    "    threshold = -12 #threshold_factor * std\n",
    "\n",
    "    spike_indices = np.where(signal < threshold)[0]\n",
    "    valid_spikes = []\n",
    "\n",
    "    for idx in spike_indices:\n",
    "        if idx - window_size < 0 or idx + window_size >= len(signal):\n",
    "            continue  # skip if window would go out of bounds\n",
    "        valid_spikes.append(idx)\n",
    "\n",
    "    waveforms = np.array([\n",
    "        signal[idx - window_size:idx + window_size]\n",
    "        for idx in valid_spikes\n",
    "    ])\n",
    "\n",
    "    return np.array(valid_spikes), waveforms\n",
    "    \n",
    "def extract_random_baseline_vpp(signal, num_samples, fs, window_ms=4):\n",
    "    \"\"\"\n",
    "    Extract random VPP values from baseline signal.\n",
    "\n",
    "    Parameters:\n",
    "    - signal (1D np.array): baseline signal for one channel.\n",
    "    - num_samples (int): number of 4 ms windows to extract (e.g., same as spike count).\n",
    "    - fs (int): sampling frequency in Hz.\n",
    "    - window_ms (float): window size in milliseconds (default: 4 ms).\n",
    "\n",
    "    Returns:\n",
    "    - vpp_list (list of floats): VPP values from baseline windows.\n",
    "    \"\"\"\n",
    "    window_samples = int(window_ms * fs / 1000)\n",
    "    max_start = len(signal) - window_samples\n",
    "\n",
    "    if max_start <= 0:\n",
    "        return []\n",
    "\n",
    "    vpp_list = []\n",
    "    for _ in range(num_samples):\n",
    "        start = np.random.randint(0, max_start)\n",
    "        window = signal[start:start + window_samples]\n",
    "        vpp = np.max(window) - np.min(window)\n",
    "        vpp_list.append(vpp)\n",
    "\n",
    "    return vpp_list\n",
    "    \n",
    "def analyze_port(df, fs, baseline_sec=50, threshold_factor=5):\n",
    "    \"\"\"\n",
    "    Analyze all electrodes in a given port and return waveform stats and metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    time = df['time']\n",
    "    df = df.drop(columns=['time'])\n",
    "\n",
    "    for ch in df.columns:\n",
    "        signal = df[ch].values\n",
    "        spike_indices, waveforms = detect_negative_spikes(signal[int((baseline_sec+5)*fs):-1], fs, threshold_factor)\n",
    "\n",
    "        if waveforms.size == 0:\n",
    "            print(f\"No spikes detected on {ch}\")\n",
    "            continue\n",
    "\n",
    "        mean_waveform = np.mean(waveforms, axis=0)\n",
    "        std_waveform = np.std(waveforms, axis=0)\n",
    "\n",
    "        spike_times = spike_indices / fs\n",
    "        duration_sec = len(signal) / fs\n",
    "\n",
    "        baseline_signal = signal[:int(fs * baseline_sec)]  # baseline\n",
    "    \n",
    "        # Now baseline VPPs:\n",
    "        n_spikes = len(spike_times)\n",
    "        baseline_vpps = extract_random_baseline_vpp(baseline_signal, n_spikes, fs)\n",
    "        metrics = {\n",
    "            'spike_count': len(spike_indices),\n",
    "            'spike_rate_Hz': len(spike_indices) / duration_sec,\n",
    "            'mean_spike_amplitude': np.mean(np.min(waveforms, axis=1)),\n",
    "            'mean_spike_peak_to_peak': np.mean(np.ptp(waveforms, axis=1)),\n",
    "            'baseline_peak_to_peak': np.mean(baseline_vpps),\n",
    "            'power_signal': np.sum(np.square(signal)),\n",
    "            'power_baseline': np.sum(np.square(baseline_signal)),\n",
    "            'rms_signal': np.sqrt(np.mean(signal ** 2)),\n",
    "            'rms_baseline': np.sqrt(np.mean(baseline_signal ** 2)),\n",
    "            'waveforms': waveforms,\n",
    "            'mean_waveform': mean_waveform,\n",
    "            'std_waveform': std_waveform,\n",
    "        }\n",
    "\n",
    "        results[ch] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_average_waveforms(metrics_dict, fs, window_ms=2, port_name='Port A'):\n",
    "    \"\"\"\n",
    "    Plot average ± std of spike waveforms for each electrode.\n",
    "    \"\"\"\n",
    "    time_vector = np.linspace(-window_ms, window_ms, 2 * int(window_ms * fs / 1000))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for ch, metrics in metrics_dict.items():\n",
    "        mean_wf = metrics['mean_waveform']\n",
    "        std_wf = metrics['std_waveform']\n",
    "        plt.plot(time_vector, mean_wf, label=ch)\n",
    "        plt.fill_between(time_vector, mean_wf - std_wf, mean_wf + std_wf, alpha=0.3)\n",
    "    \n",
    "    plt.title(f'Spike Waveforms in {port_name}')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_metrics_summary(metrics_dict, port_name='Port A'):\n",
    "    print(f\"Summary for {port_name}\")\n",
    "    for ch, m in metrics_dict.items():\n",
    "        print(f\"  {ch}: {m['spike_count']} spikes | \"\n",
    "              f\"{m['spike_rate_Hz']:.1f} Hz | \"\n",
    "              f\"Mean amp: {m['mean_spike_amplitude']:.2f} | \"\n",
    "              f\"Power: {m['power_signal']:.2f} | \"\n",
    "              f\"PtP: {m['mean_spike_peak_to_peak']:.2f}\")\n",
    "        \n",
    "def compute_average_metrics(metrics_dict):\n",
    "    \"\"\"\n",
    "    Compute average metrics across all electrodes in the given port.\n",
    "    Returns a dictionary of averaged metrics.\n",
    "    \"\"\"\n",
    "    if not metrics_dict:\n",
    "        return {}\n",
    "\n",
    "    keys = ['spike_count', 'spike_rate_Hz', 'mean_spike_amplitude', 'mean_spike_peak_to_peak', 'baseline_peak_to_peak', 'power_signal', 'power_baseline', 'rms_signal','rms_baseline' ]\n",
    "    aggregated = {k: [] for k in keys}\n",
    "\n",
    "    for ch_metrics in metrics_dict.values():\n",
    "        for k in keys:\n",
    "            aggregated[k].append(ch_metrics[k])\n",
    "\n",
    "    avg_metrics = {f'avg_{k}': np.mean(v) for k, v in aggregated.items()}\n",
    "    std_metrics = {f'std_{k}': np.std(v) for k, v in aggregated.items()}\n",
    "\n",
    "    return {**avg_metrics, **std_metrics}\n",
    "def save_average_metrics_to_csv(avg_metrics_a, avg_metrics_b, filename='spike_metrics_summary.csv'):\n",
    "    \"\"\"\n",
    "    Save average and std metrics for both ports to a CSV.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'Metric': list(avg_metrics_a.keys()),\n",
    "        'Port A': list(avg_metrics_a.values()),\n",
    "        'Port B': [avg_metrics_b.get(k, np.nan) for k in avg_metrics_a.keys()]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved summary metrics to {filename}\")\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def compare_std_metrics(metrics_dict_a, metrics_dict_b, filename=\"std_comparison_report.txt\"):\n",
    "    \"\"\"\n",
    "    Perform statistical comparison on standard deviations of metrics between two ports,\n",
    "    and save the results to a text file. The function also concludes whether the comparison \n",
    "    between ports is fair based on the similarity of the standard deviations.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict_a (dict): Per-channel metrics for Port A.\n",
    "        metrics_dict_b (dict): Per-channel metrics for Port B.\n",
    "        filename (str): Path to the output report file.\n",
    "    \"\"\"\n",
    "    keys = ['spike_count', 'spike_rate_Hz', 'mean_spike_amplitude', 'mean_spike_peak_to_peak', 'baseline_peak_to_peak', 'power_signal', 'power_baseline', 'rms_signal','rms_baseline' ]\n",
    "\n",
    "    report_lines = [\"Standard Deviation Comparison Report\",\n",
    "                    \"Comparing standard deviations between Port A and Port B:\\n\"]\n",
    "\n",
    "    # Initialize a flag to track fairness\n",
    "    is_fair_comparison = True\n",
    "\n",
    "    for k in keys:\n",
    "        stds_a = [metrics[k] for metrics in metrics_dict_a.values()]\n",
    "        print(stds_a)\n",
    "        stds_b = [metrics[k] for metrics in metrics_dict_b.values()]\n",
    "        print(stds_b)\n",
    "\n",
    "        t_stat, p_val = ttest_ind(stds_a, stds_b, equal_var=False)  # Welch’s t-test\n",
    "        result_line = f\"  {k}: t = {t_stat:.2f}, p = {p_val:.4f} {'*' if p_val < 0.05 else ''}\"\n",
    "        print(result_line)\n",
    "        report_lines.append(result_line)\n",
    "\n",
    "        # If p-value is less than 0.05, the standard deviations are significantly different\n",
    "        if p_val < 0.05:\n",
    "            is_fair_comparison = False\n",
    "\n",
    "    # Conclude if the comparison is fair based on p-value results\n",
    "    fairness_conclusion = \"Comparison is fair between Port A and Port B.\" if is_fair_comparison else \\\n",
    "                          \"Comparison is NOT fair between Port A and Port B due to significant differences in standard deviations.\"\n",
    "\n",
    "    report_lines.append(f\"\\nConclusion: {fairness_conclusion}\")\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "    print(f\"\\nReport saved to {os.path.abspath(filename)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c39fb7-27f5-447f-b272-8ede9cb9d736",
   "metadata": {},
   "source": [
    "##### Waveforms activity vs baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fca95-b6cf-4232-a804-e4461261ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming filtered_signal['Port A'] is already created\n",
    "metrics_a = analyze_port(filtered_signal['Port A'].iloc[int(0*60*fs):int(10*60*fs)], fs, baseline_sec=300) #0:240*fs \n",
    "metrics_b = analyze_port(filtered_signal['Port B'].iloc[int(0*60*fs):int(10*60*fs)], fs, baseline_sec=300)\n",
    "print('--------------------')\n",
    "plot_average_waveforms(metrics_a, fs, port_name='Port A')\n",
    "plt.savefig(f'{save_path}/{rat_id}_waveforms_PortA.svg', dpi=300)\n",
    "plot_average_waveforms(metrics_b, fs, port_name='Port B')\n",
    "plt.savefig(f'{save_path}/{rat_id}_waveforms_PortB.svg', dpi=300)\n",
    "print('--------------------')\n",
    "\n",
    "print_metrics_summary(metrics_a,port_name='Port A')\n",
    "print_metrics_summary(metrics_b,port_name='Port B')\n",
    "print('--------------------')\n",
    "\n",
    "avg_metrics_a = compute_average_metrics(metrics_a)\n",
    "avg_metrics_b = compute_average_metrics(metrics_b)\n",
    "print('--------------------')\n",
    "\n",
    "save_average_metrics_to_csv(avg_metrics_a, avg_metrics_b, filename='%s/%s_spike_metrics_summary.csv'%(save_path, rat_id))\n",
    "\n",
    "compare_std_metrics(metrics_a, metrics_b, filename='%s/%s_std_comparison.csv'%(save_path, rat_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374c6bd-4fb1-43f9-895b-54cce612715a",
   "metadata": {},
   "source": [
    "#### RMS metrics and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed21d9-43bb-4fa7-807b-51ebbcc7a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rms_per_electrode(data, fs, rat_id, start=None, end=None, save_dir='rms_outputs_electrode'):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    all_rms = []\n",
    "\n",
    "    for port in list(data.keys()):\n",
    "        df_port = data[port]\n",
    "        if port == 'Port A':\n",
    "            cuff = 'PEDOT_PSS'\n",
    "            mean_label = 'Mean_port_A'\n",
    "        elif port == 'Port B':\n",
    "            cuff = 'PEDOT_DES'\n",
    "            mean_label = 'Mean_port_B'\n",
    "        else:\n",
    "            continue  # Skip Port C (EMG)\n",
    "\n",
    "        # Apply slicing if start and end are provided\n",
    "        if start is not None and end is not None:\n",
    "            df_port = df_port.iloc[int(start*fs):int(end*fs)]\n",
    "\n",
    "        # Calculate RMS per electrode\n",
    "        rms_per_electrode = []\n",
    "        for electrode in df_port.columns:\n",
    "            if electrode.lower() == 'time':\n",
    "                continue\n",
    "            signal = df_port[electrode].values\n",
    "            rms = np.sqrt(np.mean(signal ** 2))\n",
    "            rms_per_electrode.append(rms)\n",
    "\n",
    "            all_rms.append({\n",
    "                'Rat_ID': rat_id,\n",
    "                'Condition': condition_name,\n",
    "                'Cuff': cuff,\n",
    "                'Electrode': electrode,\n",
    "                'RMS': rms\n",
    "            })\n",
    "        \n",
    "        # Mean RMS for the port\n",
    "        mean_rms_port = np.mean(rms_per_electrode)\n",
    "        all_rms.append({\n",
    "            'Rat_ID': rat_id,\n",
    "            'Condition': condition_name,\n",
    "            'Cuff': cuff,\n",
    "            'Electrode': mean_label,\n",
    "            'RMS': mean_rms_port\n",
    "        })\n",
    "\n",
    "    df_rms = pd.DataFrame(all_rms)       \n",
    "    out_path = os.path.join(save_dir, f\"{rat_id}-{start}s_to_{end}s-RMS_per_electrode.csv\")\n",
    "    df_rms.to_csv(out_path, index=False)\n",
    "    print(f\"[✓] Saved RMS data to {out_path}\")\n",
    "    return df_rms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02286d8-0e2a-4064-a274-d1377190587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and save spontaneous RMS per cuff\n",
    "# To then compute the SNR is important to ensure that both the signal and baseline are measured over the same time period \n",
    "# If they differ, the computed SNR may not accurately reflect the true ratio of signal strength to noise.\n",
    "\n",
    "start = 5*60 # 300 #60\n",
    "end = start+60 #310 #70\n",
    "rms_df = compute_rms_per_electrode(filtered_signal, fs, rat_id, save_dir=save_path, start=start, end=end) # starts and end in sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa5b92-1f75-4ff0-a8fc-b07447eec854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834307b4-1747-49df-824e-6dc399c2c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plot violin per rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc23ba0-3bf2-4704-8ea3-1f24eff7fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_rms_distribution(df_rms, rat_id):\n",
    "    # Filter out rows that are not 'Mean_port'\n",
    "    non_mean_df = rms_df[~rms_df['Electrode'].str.contains('Mean')]\n",
    "    \n",
    "    # Count A and B channels based on 'Port A_' and 'Port B_' in the 'Electrode' column\n",
    "    num_A_channels = non_mean_df['Electrode'].str.contains('Port A_').sum()\n",
    "    num_B_channels = non_mean_df['Electrode'].str.contains('Port B_').sum()\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "     # Main violin plot (no hue)\n",
    "    sns.violinplot(data=df_rms, x='Cuff', y='RMS', inner='box', color='lightgray')\n",
    "\n",
    "    # Overlay individual electrode RMS values (colored)\n",
    "    sns.stripplot(data=df_rms, x='Cuff', y='RMS',\n",
    "                  hue='Electrode', palette='tab10',\n",
    "                  dodge=True, jitter=True, alpha=0.8, size=6)\n",
    "\n",
    "    # Annotate number of channels on top of each violin\n",
    "    plt.text(x=0, y=df_rms['RMS'].min(), s=f\"n={num_A_channels}\", \n",
    "             ha='center', va='bottom', fontsize=20)\n",
    "    plt.text(x=1, y=df_rms['RMS'].min(), s=f\"n={num_B_channels}\", \n",
    "             ha='center', va='bottom', fontsize=20)\n",
    "    \n",
    "    plt.title(f\"RMS per Electrode by Cuff ({rat_id})\")\n",
    "    plt.ylabel(\"RMS\")\n",
    "    plt.xlabel(\"Cuff Type\")\n",
    "    plt.legend(title='Electrode', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_rms_distribution(rms_df, rat_id)\n",
    "plt.savefig(f'{save_path}/{rat_id}_RMS-{start}s_to_{end}s.svg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fc636-61cd-4807-99eb-ba0cc3e1a374",
   "metadata": {},
   "source": [
    "### Load and analysed pre-saved stimulation pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373718f5-a4ae-435f-90d2-8fff2474114d",
   "metadata": {},
   "source": [
    "Load pre-saved pkl forom saved_stim_pkls folder, and then computed RMS from baseline and evoked activity (from 0.015s to 0.2s after stim artifact based on preliminary analysis). Saves the plots and cvs reports in the same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb0568-00f6-4323-897c-3659cf38cae8",
   "metadata": {},
   "source": [
    "#### Definitions for extracting waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24022fe-23b1-4eed-89fc-7a0abb922fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rms(signal):\n",
    "    \"\"\"Compute RMS of a 1D NumPy array.\"\"\"\n",
    "    return np.sqrt(np.mean(signal**2))\n",
    "\n",
    "def extract_max(signal):\n",
    "    \"\"\"Compute Max of a 1D NumPy array.\"\"\"\n",
    "    return np.max(signal)\n",
    "    \n",
    "def extract_auc_positive(signal, sampling_rate=1.0):\n",
    "    \"\"\"\n",
    "    Compute the area under the curve (AUC) for the positive portion of a 1D signal.\n",
    "    \n",
    "    Parameters:\n",
    "        signal (np.ndarray): Input signal.\n",
    "        sampling_rate (float): Sampling rate in Hz (default 1.0 for unitless AUC).\n",
    "\n",
    "    Returns:\n",
    "        float: AUC of the positive part of the signal.\n",
    "    \"\"\"\n",
    "    positive_signal = np.where(signal > 0, signal, 0)\n",
    "    dt = 1.0 / sampling_rate\n",
    "    return np.trapz(positive_signal, dx=dt)\n",
    "def find_pulse_artifacts(emg, time, fs, save_path, threshold_factor=30, plot=True, title=\"EMG Artifact Detection\"):\n",
    "    \"\"\"\n",
    "    Detect stimulation artifacts in EMG and optionally plot the signal and detection threshold.\n",
    "\n",
    "    Args:\n",
    "        emg (np.array): EMG signal (1D).\n",
    "        fs (float): Sampling frequency in Hz.\n",
    "        threshold_factor (float): Multiplier for baseline SD to set detection threshold.\n",
    "        plot (bool): Whether to plot EMG signal with detection threshold and artifacts.\n",
    "        title (str): Title for the plot.\n",
    "\n",
    "    Returns:\n",
    "        pulses (list): List of artifact indices (rising edge).\n",
    "    \"\"\"\n",
    "    valid_emg = emg[(emg < 1000) & (emg > -1000)]\n",
    "    max_emg = np.max(valid_emg)\n",
    "    min_emg_interval1 = np.min(valid_emg[int(10*fs):int(15*fs)])\n",
    "    min_emg_interval2 = np.min(valid_emg[int(25*fs):int(30*fs)])\n",
    "    thresh =  0.7*np.min([np.abs(min_emg_interval1), np.abs(min_emg_interval2)]) \n",
    "    \n",
    "    # Detect threshold crossings (rising edge) on inverted signal\n",
    "    crossings = np.where(\n",
    "        (-emg[:-1] < thresh) &               \n",
    "        (-emg[1:] >= thresh) &               \n",
    "        (-emg[1:] < 1500)                    # discard if overly negative\n",
    "    )[0] + 1\n",
    "\n",
    "    # Keep only one index per pulse (ensure separation)\n",
    "    min_separation = int(0.8 * fs)\n",
    "    pulses = []\n",
    "    for idx in crossings:\n",
    "        if not pulses or (idx - pulses[-1]) > min_separation:\n",
    "            pulses.append(idx)    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(time, emg, label='EMG')\n",
    "        plt.axhline(-thresh, color='red', linestyle='--', label=f'Threshold = {threshold_factor} × SD')\n",
    "        plt.scatter(np.array(pulses)/fs, emg[pulses], color='orange', label='Detected Artifacts')\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "\n",
    "    return pulses\n",
    "\n",
    "def plot_waveforms_after_artifacts(emg, fs, pulse_idxs, save_path, title=\"EMG Waveforms\"):\n",
    "    \"\"\"\n",
    "    Superplot waveforms of EMG from right after each detected artifact for 1 second.\n",
    "    \n",
    "    Args:\n",
    "        emg (np.array): EMG signal (1D).\n",
    "        fs (float): Sampling frequency in Hz.\n",
    "        pulse_idxs (list): Indices of detected artifacts.\n",
    "        title (str): Title for the plot.\n",
    "    \"\"\"\n",
    "    # Define the time window for extracting the waveform: from 0.0001s after the artifact to 1 second after it\n",
    "    start_time = 0  # Time in seconds to start after the artifact\n",
    "    end_time = 0.2  # Time in seconds to end after the artifact\n",
    "    start_sample = int(start_time * fs)  # Convert to sample index\n",
    "    end_sample = int(end_time * fs)  # Convert to sample index\n",
    "\n",
    "    snippets_1 = []\n",
    "    snippets_2 = []\n",
    "\n",
    "    # Extract snippets of EMG signal after each artifact (up to 10 pulses) - and then next 10 pulses\n",
    "    for idx in pulse_idxs[:10]:  # Get waveforms for the first 10 pulses\n",
    "        # Check if there's enough signal before and after the artifact\n",
    "        if idx + start_sample >= 0 and idx + end_sample < len(emg):\n",
    "            snippets_1.append(emg[idx + start_sample: idx + end_sample])  # Extract the snippet after the artifact\n",
    "\n",
    "    for idx in pulse_idxs[11:20]:  # Get waveforms for the next 10 pulses\n",
    "        # Check if there's enough signal before and after the artifact\n",
    "        if idx + start_sample >= 0 and idx + end_sample < len(emg):\n",
    "            snippets_2.append(emg[idx + start_sample: idx + end_sample])  # Extract the snippet after the artifact\n",
    "\n",
    "    snippets_1 = np.array(snippets_1)\n",
    "    snippets_2 = np.array(snippets_2)\n",
    "\n",
    "    # Calculate mean waveform and standard deviation\n",
    "    mean_wf_1 = snippets_1.mean(0)\n",
    "    std_wf_1 = snippets_1.std(0)\n",
    "\n",
    "    mean_wf_2 = snippets_2.mean(0)\n",
    "    std_wf_2 = snippets_2.std(0)\n",
    "\n",
    "    # Adjust time vector to match the length of mean_wf\n",
    "    tvec = np.arange(0, len(mean_wf_1)) / fs  # Time vector adjusted to match the length of the waveform\n",
    "    try:\n",
    "        # Plot the mean waveform with ± 1 standard deviation\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(tvec, mean_wf_1, label='Mean waveform', color='blue')\n",
    "        plt.fill_between(tvec, mean_wf_1 - std_wf_1, mean_wf_1 + std_wf_1, alpha=0.3, color='blue', label='± 1 SD')\n",
    "        plt.plot(tvec, mean_wf_2, label='Mean waveform', color='orange')\n",
    "        plt.fill_between(tvec, mean_wf_2 - std_wf_2, mean_wf_2 + std_wf_2, alpha=0.3, color='orange', label='± 1 SD')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.ylim([-100,100])\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    except:\n",
    "        print('Not enough pulses')\n",
    "\n",
    "# Define the band-pass filter function\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, data)\n",
    "    \n",
    "def analyze_evoked_emg_folder(pkl_folder, rat_id, output_folder=None, fs_override=None):\n",
    "    \"\"\"\n",
    "    For every .pkl in pkl_folder, load Port C EMG and:\n",
    "      • detect pulse artifacts\n",
    "      • compute RMS of baseline (first 5 s) and each 10 s epoch\n",
    "      • extract ± windowed snippets around each pulse\n",
    "      • superplot waveforms\n",
    "      • test activation (>3×SD baseline)\n",
    "      • save per‐file CSV of all 11 RMS + mean/std + activation count\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder or pkl_folder, exist_ok=True)\n",
    "    print(pkl_folder)\n",
    "    for fname in os.listdir(pkl_folder):\n",
    "        if not fname.lower().endswith('.pkl'):\n",
    "            continue\n",
    "        path = os.path.join(pkl_folder, fname)\n",
    "        with open(path, 'rb') as f:\n",
    "            stim_data_dict, fs, port_info, full_df = pickle.load(f)\n",
    "            print(fs)\n",
    "        if fs_override:\n",
    "            fs = fs_override\n",
    "        print('=----------------------------------------')\n",
    "        print(fname)\n",
    "        \n",
    "        # get Port C as a 1D numpy array\n",
    "        if 'Port C' in stim_data_dict and stim_data_dict['Port C'] is not None:\n",
    "            port_c_df = stim_data_dict['Port C']\n",
    "            raw_emg = port_c_df.iloc[:, 0].values  # assume column 1 is time\n",
    "            time = port_c_df.iloc[:, 1].values \n",
    "        else:\n",
    "            # fallback to full_df\n",
    "            raw_emg = full_df['Port C'].iloc[:,0].values\n",
    "        \n",
    "        # Apply the band-pass filter to the EMG signal (30–500 Hz)\n",
    "        lowcut = 60  # 30 Hz low cut\n",
    "        highcut = 500  # 500 Hz high cut\n",
    "        emg = bandpass_filter(raw_emg, lowcut, highcut, fs)\n",
    "\n",
    "        # detect pulses\n",
    "        pulse_idxs = find_pulse_artifacts(emg, time, fs, os.path.join(output_folder or pkl_folder, fname.replace('.pkl','') + '_signal_artifacts.svg'))\n",
    "\n",
    "        # define epochs: baseline = first 8s; then up to 20 pulses @1Hz (~20 s)\n",
    "        baseline_epoch = (2, int(2.2 * fs))\n",
    "        stim_epochs = []\n",
    "        for i, idx in enumerate(pulse_idxs[:20]):\n",
    "            start = idx + int(0.011*fs) # right after stim artifact  # checked this is when stim artifact finishes\n",
    "            end = start + int(0.2 * fs)  # 0.2‐second window per pulse\n",
    "            stim_epochs.append((start, end))\n",
    "\n",
    "        # compute RMS values\n",
    "        rms_values = []\n",
    "        auc_values = []\n",
    "        max_values = []\n",
    "        \n",
    "        # baseline\n",
    "        bdata = emg[baseline_epoch[0]:baseline_epoch[1]]\n",
    "        rms_values.append(extract_rms(bdata))\n",
    "        auc_values.append(extract_auc_positive(bdata))\n",
    "        max_values.append(extract_max(bdata))\n",
    "        \n",
    "        # stim pulses\n",
    "        for (s,e) in stim_epochs:\n",
    "            rms_values.append(extract_rms(emg[s:e]))\n",
    "            auc_values.append(extract_auc_positive(emg[s:e]))\n",
    "            max_values.append(extract_max(emg[s:e]))\n",
    "\n",
    "        # average & std\n",
    "        rms_arr = np.array(rms_values)\n",
    "        auc_arr = np.array(auc_values)\n",
    "        max_arr = np.array(max_values)\n",
    "        mean_rms, std_rms = rms_arr.mean(), rms_arr.std()\n",
    "        mean_auc, std_auc = auc_arr.mean(), auc_arr.std()\n",
    "        mean_max, std_max = max_arr.mean(), max_arr.std()\n",
    "\n",
    "        # activation counts (>3×SD baseline) - there is significant difference in after stim EMG compared to baseline (3SD)\n",
    "        act_thresh = 3 * np.std(bdata)\n",
    "        activations = [(emg[s:e].max() > act_thresh) for s,e in stim_epochs]\n",
    "        n_activation = sum(activations)\n",
    "\n",
    "        plot_waveforms_after_artifacts(emg, fs, pulse_idxs, os.path.join(output_folder or pkl_folder, fname.replace('.pkl','') + '_emg_waveforms.svg'))\n",
    "        \n",
    "        # save results\n",
    "        res = {\n",
    "            'epoch':['baseline'] + [f'pulse_{i+1}' for i in range(len(stim_epochs))],\n",
    "            'RMS': list(rms_arr),\n",
    "            'AUC': list(auc_arr),\n",
    "            'MAX': list(max_arr),\n",
    "            'activation':[False] + activations  # baseline no activation\n",
    "        }\n",
    "        df_res = pd.DataFrame(res)\n",
    "        df_res.loc['summary_mean'] = {\n",
    "            'epoch':'summary_mean',\n",
    "            'RMS':mean_rms,\n",
    "            'AUC': mean_auc,\n",
    "            'MAX': mean_max,\n",
    "            'activation_count':n_activation\n",
    "        }\n",
    "        df_res.loc['summary_std'] = {\n",
    "            'epoch':'summary_std',\n",
    "            'RMS':std_rms,\n",
    "            'AUC':std_auc,\n",
    "            'MAX': std_max,\n",
    "            'activation_count':n_activation\n",
    "        }\n",
    "        out_csv = os.path.join(output_folder or pkl_folder,\n",
    "                               fname.replace('.pkl','') + '_emg_analysis.csv')\n",
    "        df_res.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved analysis for {fname} → {out_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518dc0c-2a1e-4b88-a41d-cb0b7904f122",
   "metadata": {},
   "source": [
    "#### Extract waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dfba29-06ea-4ab7-83ab-fcc5a0ccab65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------------------------\n",
    "base_path = \"%s/%s/rhs_recordings\"%(experiments_path, 'rat1-RightNerve' ) #'rat4-LeftNerve')\n",
    "rat_id = 'Rat_01-RN' \n",
    "stim_load_path = f'{base_path}/saved_stim_pkls/'\n",
    "analyze_evoked_emg_folder(stim_load_path, rat_id, output_folder=f'{experiments_path}/stimulation/{rat_id}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2da73-5608-47aa-b6ea-e04b37e8addb",
   "metadata": {},
   "source": [
    "#### Definitions for creating overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9b0d5-87e4-4900-ba61-28d67428b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_and_validate_df(df, fname):\n",
    "    # Check for required columns\n",
    "    required_cols = {'epoch', 'RMS', 'activation'}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        print(f\" Missing columns in {fname}, skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    # Extract baseline row\n",
    "    baseline_row = df[df['epoch'].str.lower() == 'baseline']\n",
    "    baseline_rms = baseline_row['RMS'].values[0] if not baseline_row.empty else 0\n",
    "    baseline_auc = baseline_row['AUC'].values[0] if not baseline_row.empty else 0\n",
    "    baseline_max = baseline_row['MAX'].values[0] if not baseline_row.empty else 0\n",
    "\n",
    "    # Keep only rows matching pulse_1 to pulse_20\n",
    "    pulse_rows = df[df['epoch'].str.match(r'^pulse_\\d+$', na=False)].copy()\n",
    "\n",
    "    if len(pulse_rows) != 20:\n",
    "        print(f\" File has {len(pulse_rows)} valid pulses instead of 20: {fname}\")\n",
    "        return None, 0, 0, 0\n",
    "\n",
    "    pulse_rows['pulse_num'] = pulse_rows['epoch'].str.extract(r'pulse_(\\d+)').astype(int)\n",
    "    return pulse_rows, baseline_rms, baseline_auc, baseline_max\n",
    "\n",
    "\n",
    "def parse_filename(fname):\n",
    "    parts = fname.replace('.csv', '').split('_')\n",
    "\n",
    "    current = None\n",
    "    port = None\n",
    "    duration = None\n",
    "\n",
    "    for part in parts:\n",
    "        if re.match(r'^\\d+\\.?\\d*uA$', part):\n",
    "            current = part\n",
    "        elif 'Port' in part or 'port' in part:\n",
    "            port = part\n",
    "        elif re.match(r'^\\d+us$', part):\n",
    "            duration = part\n",
    "        else:\n",
    "            duration = '100us'\n",
    "\n",
    "    return current, port, duration\n",
    "\n",
    "\n",
    "def load_and_melt_emg_reports(folder, n_ch, twitch_th, side_nerve):\n",
    "    records = []\n",
    "\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname.startswith('overview'):\n",
    "            continue\n",
    "        if not fname.endswith('.csv'):\n",
    "            continue\n",
    "        path = os.path.join(folder, fname)\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "        except Exception as e:\n",
    "            print(f\" Error loading {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"📄 Found file: {fname}\")\n",
    "\n",
    "        # Clean and extract pulses\n",
    "        pulses, baseline_rms, baseline_auc, baseline_max = clean_and_validate_df(df, fname)\n",
    "        if pulses is None:\n",
    "            continue\n",
    "\n",
    "        # Extract metadata from filename\n",
    "        current, port, duration = parse_filename(fname)\n",
    "        if not all([current, port, duration]):\n",
    "            print(f\" Could not parse all metadata from: {fname}\")\n",
    "            continue\n",
    "\n",
    "        for _, row in pulses.iterrows():\n",
    "            # Extract current value\n",
    "            match = re.match(r\"(\\d+)\", current)\n",
    "            if match:\n",
    "                value = float(match.group(1))\n",
    "            else:\n",
    "                value = None  # or continue, depending on how you want to handle this\n",
    "        \n",
    "            pulse_num = int(row['pulse_num'])\n",
    "        \n",
    "            # Assign nerve side and threshold depending on port and pulse number\n",
    "            if port == 'PortA' or port == 'portA' :\n",
    "                side = side_nerve[0]\n",
    "                threshold = twitch_th[0] if pulse_num <= 10 else twitch_th[1]\n",
    "            elif port == 'PortB' or port == 'portB':\n",
    "                side = side_nerve[1]\n",
    "                threshold = twitch_th[1] if pulse_num <= 10 else twitch_th[0]\n",
    "            else:\n",
    "                side = None\n",
    "                threshold = None\n",
    "        \n",
    "            # Determine twitch\n",
    "            twitch = 'Yes' if value is not None and value > threshold else 'No'  \n",
    "            \n",
    "            records.append({\n",
    "                'file': fname,\n",
    "                'side_nerve': side,\n",
    "                'number_ch': n_ch,\n",
    "                'current': current,\n",
    "                'port': port,\n",
    "                'duration': duration,\n",
    "                'pulse': row['pulse_num'],\n",
    "                'RMS': row['RMS'],\n",
    "                'AUC': row['AUC'],\n",
    "                'MAX': row['MAX'],\n",
    "                'activation': row['activation'],\n",
    "                'twitch': twitch,\n",
    "                'baseline_RMS': baseline_rms,\n",
    "                'baseline_AUC': baseline_auc,\n",
    "                'baseline_MAX': baseline_max\n",
    "            })\n",
    "\n",
    "    df_all = pd.DataFrame.from_records(records)\n",
    "    if df_all.empty:\n",
    "        print(\" No data extracted.\")\n",
    "        return None\n",
    "\n",
    "    print(\"✅ Loaded Data Columns:\", df_all.columns)\n",
    "    print(\"✅ Unique ports:\", df_all['port'].unique())\n",
    "    return df_all\n",
    "\n",
    "def plot_rms_traces(df_long):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    g = sns.FacetGrid(df_long, col=\"port\", height=4, aspect=1.3)\n",
    "    g.map_dataframe(\n",
    "        sns.lineplot,\n",
    "        x=\"pulse\", y=\"RMS\",\n",
    "        hue=\"current\", marker=\"o\"\n",
    "    )\n",
    "    g.set_titles(\"First Port: {col_name}\")\n",
    "    g.add_legend(title=\"Amplitude (µA)\")\n",
    "    g.set_axis_labels(\"Pulse Number\", \"RMS (µV)\")\n",
    "    plt.ylim([0, 100])\n",
    "    plt.suptitle(\"EMG RMS over 20 Pulses\\n(Grouped by FirstPort and Amplitude)\", y=1.05, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757146d-1335-4291-8f65-92d12808f217",
   "metadata": {},
   "source": [
    "#### Create overview file and plot RMS all pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e93a85-1c22-4271-9671-55af70ac9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "folder = f'{experiments_path}/stimulation/{rat_id}' \n",
    "print(folder)\n",
    "#----------------------------------\n",
    "# 1st amplitudes to start seing a motor response in Port A, second in Port B\n",
    "twitch_th = [6, 9]   # R1_RN: [6, 8.5] // R1_LN: [8, 13] // R2_RN: [6, 35] // R2_LN: [32, 5] // R3_RN: [8, 9] // R3_LN: [3, 2] // R4_RN: [7, 9] // L4_LN: [8, 10] \n",
    "# Location in nerve: first DES, 2nd PSS\n",
    "side_nerve = ['Right', 'Left'] # For Rats 1 and 2\n",
    "#side_nerve = ['Left', 'Right'] # For Rats 3 and 4\n",
    "\n",
    "# Number of active channels to stimulate both ports\n",
    "n_ch = 7       #  R1_RN: 7  // R1_LN: 7 // R2_RN: 8 // R2_LN: 10 // R3_RN: 5 // R3_LN: 7 // R4_RN:  // R4_LN: 5\n",
    "\n",
    "#-------------------------\n",
    "\n",
    "df_long = load_and_melt_emg_reports(folder, n_ch, twitch_th, side_nerve)\n",
    "df_long.to_csv(f'{folder}/overview_stim.csv', index=False)\n",
    "\n",
    "if not df_long.empty:\n",
    "    plot_rms_traces(df_long)\n",
    "    plt.savefig(f'{folder}/RMS_vs_pulse.svg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a12d54-322d-45b7-abcd-e5b26eb17a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- STEP 1: Assign 'material' and 'order' ---- #\n",
    "def get_material(row):\n",
    "    first_material = 'PEDOT:DES' if row['port'] == 'PortA' else 'PEDOT:pSS'  # for pilot: '1stPortA' and PSS\n",
    "    second_material = 'PEDOT:PSS' if first_material == 'PEDOT:DES' else 'PEDOT:DES'\n",
    "    return first_material if row['pulse'] <= 10 else second_material\n",
    "\n",
    "df_long['port'] = df_long['port'].replace({'portA': 'PortA', 'portB': 'PortB'})\n",
    "\n",
    "df_long['material'] = df_long.apply(get_material, axis=1)\n",
    "\n",
    "df_long['order'] = df_long['port'].map({\n",
    "    'PortA': 'DES first',\n",
    "    'PortB': 'PSS first'\n",
    "})\n",
    "\n",
    "# Convert current to numeric (e.g. \"15uA\" → 15)\n",
    "df_long['current_uA'] = df_long['current'].str.replace('uA', '').astype(float)\n",
    "\n",
    "# ---- STEP 2: Create unique identifier for each file ---- #\n",
    "# Create file_id based on file names (assuming 'file' column exists)\n",
    "if 'file' in df_long.columns:\n",
    "    df_long['file_id'] = df_long.groupby(['file']).ngroup()  # Assigning a unique ID for each file\n",
    "else:\n",
    "    print(\"Error: 'file' column is missing in df_long\")\n",
    "\n",
    "# ---- STEP 3: Aggregate by current, material, order, and file_id ---- #\n",
    "agg_funcs = {\n",
    "    'RMS': ['mean', 'sem'],\n",
    "    'AUC': ['mean', 'sem'],\n",
    "    'MAX': ['mean', 'sem'],\n",
    "    'baseline_RMS': 'mean',\n",
    "    'baseline_AUC': 'mean',\n",
    "    'baseline_MAX': 'mean'\n",
    "}\n",
    "\n",
    "# Perform aggregation, but keep file_id in the grouping\n",
    "plot_data = df_long.groupby(['current_uA', 'material', 'order', 'file_id']).agg(agg_funcs)\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "plot_data.columns = ['_'.join(col).strip() for col in plot_data.columns.values]\n",
    "plot_data = plot_data.reset_index()\n",
    "\n",
    "# ---- STEP 4: Plotting ---- #\n",
    "metrics = ['RMS', 'AUC', 'MAX']\n",
    "colors = {\n",
    "    'PEDOT:PSS': 'tab:blue',\n",
    "    'PEDOT:DES': 'tab:orange'\n",
    "}\n",
    "\n",
    "# Initialize the figure and axes\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "\n",
    "# Loop over each metric (RMS, AUC, MAX)\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axs[i]\n",
    "\n",
    "    # Loop over each stimulation order (e.g., PEDOT:PSS first or PEDOT:DES first)\n",
    "    for order in plot_data['order'].unique():\n",
    "        subset = plot_data[plot_data['order'] == order]\n",
    "\n",
    "        # Loop over materials (e.g., PEDOT:PSS and PEDOT:DES)\n",
    "        for material in ['PEDOT:PSS', 'PEDOT:DES']:\n",
    "            mat_data = subset[subset['material'] == material]\n",
    "            if mat_data.empty:\n",
    "                continue\n",
    "\n",
    "            linestyle = '-' if order == 'PSS first' else '--'  # Decide on linestyle for material\n",
    "            label = f'{material} ({order})'\n",
    "\n",
    "            # Plot the data, with different markers based on 'file_id'\n",
    "            for file_id in mat_data['file_id'].unique():\n",
    "                file_data = mat_data[mat_data['file_id'] == file_id]\n",
    "                ax.errorbar(\n",
    "                    file_data['current_uA'], file_data[f'{metric}_mean'],\n",
    "                    yerr=file_data[f'{metric}_sem'],\n",
    "                    label=None,  # We handle labels later\n",
    "                    color=colors[material],  # Ensure different colors for each material\n",
    "                    linestyle=linestyle,\n",
    "                    marker = 'o' if order == 'PSS first' else 's',\n",
    "                    markersize=8,  # Adjust marker size if needed\n",
    "                    alpha=0.7  # Adjust transparency to distinguish overlapping points\n",
    "                )\n",
    "\n",
    "    # Add baseline and 3×baseline lines (only once per metric)\n",
    "    overall_baseline = df_long[f'baseline_{metric}'].mean()\n",
    "    baseline_std = df_long[f'baseline_{metric}'].std()\n",
    "    ax.text(0.02, 0.95, f'Baseline μ±σ:\\n{overall_baseline:.2f} ± {baseline_std:.2f}',\n",
    "            transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.5))\n",
    "    ax.axhline(overall_baseline, color='red', linestyle=':', linewidth=2, label='Baseline')\n",
    "    #ax.axhline(1.5 * overall_baseline, color='red', linestyle='--', linewidth=2, label='1.5× Baseline')\n",
    "    ax.axvline(twitch_th[0], color='orange', linestyle='--', linewidth=2, label='Twitch threshol Port A')\n",
    "    ax.axvline(twitch_th[1], color='blue', linestyle='--', linewidth=2, label='Twitch threshol Port B')\n",
    "\n",
    "    # Add baseline labels (only once per subplot)\n",
    "    if i == 0:\n",
    "        ax.annotate('Baseline', xy=(0.95, overall_baseline), xycoords=('axes fraction', 'data'),\n",
    "                    xytext=(-10, -5), textcoords='offset points',\n",
    "                    ha='right', va='top', color='red', fontsize=12)\n",
    "\n",
    "    # Customize titles and labels for each subplot\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('Current (uA)')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add a unified legend and ensure it's visible\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))  # This removes duplicates\n",
    "\n",
    "# Place the legend at the top of the plot\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Custom legend handles for stimulation order markers\n",
    "circle_marker = mlines.Line2D([], [], color='black', marker='o', linestyle='None',\n",
    "                              markersize=8, label='PSS first (circle)')\n",
    "square_marker = mlines.Line2D([], [], color='black', marker='s', linestyle='None',\n",
    "                              markersize=8, label='DES first (square)')\n",
    "# Combine existing legend labels with marker shape legend\n",
    "custom_handles = list(by_label.values()) + [circle_marker, square_marker]\n",
    "custom_labels = list(by_label.keys()) + ['PSS first (circle)', 'DES first (square)']\n",
    "\n",
    "fig.legend(custom_handles, custom_labels, loc='upper center', ncol=4, bbox_to_anchor=(0.5, 0.1))\n",
    "fig.legend(custom_handles, custom_labels, loc='upper center', ncol=4, bbox_to_anchor=(0.5, 0.1))\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)  # Adjust the layout to give space for the legend\n",
    "plt.show()\n",
    "plt.savefig(f'{folder}/stim_vs_current.svg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf18b3a-44cc-4b97-8eca-35764f343ca9",
   "metadata": {},
   "source": [
    "### Population analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bf590-368f-482c-a67f-d36cf9d71513",
   "metadata": {},
   "source": [
    "#### Population SNR (average baseline vs activity for each port, 60 sec intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d42123-51bd-489c-8ffa-fa39d3a59a09",
   "metadata": {},
   "source": [
    "This ratio normalizes each animal to its own baseline, accounting for absolute differences in signal strength or noise floor.\n",
    "Since both numerator and denominator are from the same electrodes, hardware, and conditions, many inter-animal differences cancel out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4d88d-582c-4ead-8a3d-3597ad9ccf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Define your folder path\n",
    "condition = 'evoked' \n",
    "directory =  \"../PEDOT-DES/%s\"%condition  # change if needed\n",
    "\n",
    "# Collect all relevant files\n",
    "all_files = os.listdir(directory)\n",
    "\n",
    "baseline_files = [f for f in all_files if \"baseline\" in f]\n",
    "activity_files = [f for f in all_files if \"activity\" in f]\n",
    "\n",
    "# Good channels\n",
    "good_channels = {1, 2, 3, 13, 18, 19, 29, 31, 22,10,21,11,28,4,27,26,24,7}  # small channels only\n",
    "\n",
    "\n",
    "# Helper to extract channel number from Electrode string\n",
    "def extract_channel_num(electrode):\n",
    "    match = re.search(r'[A-Z]-0*(\\d+)$', electrode)\n",
    "    return int(match.group(1)) if match else None\n",
    "    \n",
    "# Helper to get rat ID\n",
    "def get_rat_id(filename):\n",
    "    parts = filename.split(\"-\")\n",
    "    return \"-\".join(parts[:2])  # e.g., 'Rat_01-RN'\n",
    "\n",
    "# Match baseline and activity by rat ID\n",
    "rat_ids = list(set(get_rat_id(f) for f in baseline_files))\n",
    "snr_data = []\n",
    "\n",
    "print(rat_ids)\n",
    "\n",
    "# Map rat_id -> filename\n",
    "baseline_dict = {get_rat_id(f): f for f in baseline_files if get_rat_id(f)}\n",
    "activity_dict = {get_rat_id(f): f for f in activity_files if get_rat_id(f)}\n",
    "\n",
    "# Match only rats with both files\n",
    "common_rats = baseline_dict.keys() & activity_dict.keys()\n",
    "\n",
    "snr_data = []\n",
    "\n",
    "snr_data = []\n",
    "\n",
    "for rat_id in sorted(common_rats):\n",
    "    baseline_df = pd.read_csv(os.path.join(directory, baseline_dict[rat_id]))\n",
    "    activity_df = pd.read_csv(os.path.join(directory, activity_dict[rat_id]))\n",
    "    \n",
    "    # Apply channel number extraction to both datasets\n",
    "    for df in [baseline_df, activity_df]:\n",
    "        df[\"channel_num\"] = df[\"Electrode\"].apply(extract_channel_num)\n",
    "\n",
    "    # Initialize a dictionary to store the filtered data for both ports\n",
    "    filtered_data = {\"A\": {\"baseline\": None, \"activity\": None}, \"B\": {\"baseline\": None, \"activity\": None}}\n",
    "\n",
    "    # Filter baseline and activity data for both ports A and B\n",
    "    for port in [\"A\", \"B\"]:\n",
    "        port_str = f\"Port {port}\"\n",
    "        is_port = lambda df: df[\"Electrode\"].str.startswith(port_str)\n",
    "        is_good = lambda df: df[\"channel_num\"].isin(good_channels)\n",
    "\n",
    "        # Filter baseline and activity data for the good channels of this port\n",
    "        baseline_filtered = baseline_df[is_port(baseline_df) & is_good(baseline_df)]\n",
    "        activity_filtered = activity_df[is_port(activity_df) & is_good(activity_df)]\n",
    "\n",
    "        # Store the filtered data for both baseline and activity\n",
    "        filtered_data[port][\"baseline\"] = baseline_filtered\n",
    "        filtered_data[port][\"activity\"] = activity_filtered\n",
    "\n",
    "    # Now, we trim both ports to the same number of channels (minimum number of good channels)\n",
    "    min_channels = min(len(filtered_data[\"A\"][\"baseline\"]), len(filtered_data[\"B\"][\"baseline\"]),\n",
    "                       len(filtered_data[\"A\"][\"activity\"]), len(filtered_data[\"B\"][\"activity\"]))\n",
    "\n",
    "    # Trim data for both ports based on the minimum number of good channels\n",
    "    baseline_filtered_A = filtered_data[\"A\"][\"baseline\"].head(min_channels)\n",
    "    baseline_filtered_B = filtered_data[\"B\"][\"baseline\"].head(min_channels)\n",
    "    activity_filtered_A = filtered_data[\"A\"][\"activity\"].head(min_channels)\n",
    "    activity_filtered_B = filtered_data[\"B\"][\"activity\"].head(min_channels)\n",
    "\n",
    "    # Calculate SNR if there's enough data\n",
    "    if not baseline_filtered_A.empty and not baseline_filtered_B.empty and \\\n",
    "       not activity_filtered_A.empty and not activity_filtered_B.empty:\n",
    "        baseline_rms_A = baseline_filtered_A[\"RMS\"].mean()\n",
    "        baseline_rms_B = baseline_filtered_B[\"RMS\"].mean()\n",
    "        activity_rms_A = activity_filtered_A[\"RMS\"].mean()\n",
    "        activity_rms_B = activity_filtered_B[\"RMS\"].mean()\n",
    "\n",
    "        # Calculate the SNR for both ports\n",
    "        snr_A = activity_rms_A / baseline_rms_A if baseline_rms_A != 0 else None\n",
    "        snr_B = activity_rms_B / baseline_rms_B if baseline_rms_B != 0 else None\n",
    "\n",
    "        # Append results for both ports\n",
    "        snr_data.append({\n",
    "            \"Rat\": rat_id,\n",
    "            \"Port\": \"Port A\",\n",
    "            \"SNR\": snr_A,\n",
    "            \"GoodChannelsUsed\": min_channels\n",
    "        })\n",
    "        snr_data.append({\n",
    "            \"Rat\": rat_id,\n",
    "            \"Port\": \"Port B\",\n",
    "            \"SNR\": snr_B,\n",
    "            \"GoodChannelsUsed\": min_channels\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "snr_df = pd.DataFrame(snr_data)\n",
    "print(snr_df)\n",
    "output_path = os.path.join(directory, f\"SNR_fair_summary_{condition}.csv\")\n",
    "snr_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "# Plot paired SNR comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.pointplot(data=snr_df, x=\"Port\", y=\"SNR\", hue=\"Rat\", markers=\"o\", dodge=True, join=True)\n",
    "plt.title(\"Paired SNR Comparison Between Port A and Port B\")\n",
    "plt.ylabel(\"SNR (Activity / Baseline RMS)\")\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Rat\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{directory}/paired_fair_SNR_{condition}.svg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Pivot to wide format: SNR_Port_A and SNR_Port_B per rat\n",
    "df_wide = snr_df.pivot(index=\"Rat\", columns=\"Port\", values=\"SNR\").reset_index()\n",
    "df_wide.columns.name = None  # clean up\n",
    "\n",
    "# Add ΔSNR column\n",
    "df_wide[\"Delta_SNR\"] = df_wide[\"Port A\"] - df_wide[\"Port B\"]\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 8), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "# ---- Plot 1: Paired SNR with error bars ----\n",
    "for i, row in df_wide.iterrows():\n",
    "    axs[0].plot([\"Port A\", \"Port B\"], [row[\"Port A\"], row[\"Port B\"]],\n",
    "                marker=\"o\", label=f'{row[\"Rat\"]})')\n",
    "\n",
    "axs[0].set_ylabel(\"SNR\")\n",
    "axs[0].set_title(\"SNR per Port (paired per rat)\")\n",
    "axs[0].legend(loc=\"upper right\", fontsize=8, ncol=2)\n",
    "\n",
    "# Summary error bars (mean ± SEM)\n",
    "means = df_wide[[\"Port A\", \"Port B\"]].mean()\n",
    "sems = df_wide[[\"Port A\", \"Port B\"]].sem()\n",
    "axs[0].errorbar([\"Port A\", \"Port B\"], means, yerr=sems, fmt='o', color='black', capsize=5, lw=2, label=\"Mean ± SEM\")\n",
    "\n",
    "# ---- Plot 2: ΔSNR subplot ----\n",
    "sns.barplot(x=\"Rat\", y=\"Delta_SNR\", data=df_wide, ax=axs[1], palette=\"coolwarm\", edgecolor=\"black\")\n",
    "axs[1].axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "axs[1].set_ylabel(\"ΔSNR (A - B)\")\n",
    "axs[1].set_title(\"SNR Difference per Rat\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{directory}/SNR_fair_increment_{condition}.svg', dpi=300)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df_wide[[\"Port A\", \"Port B\"]].describe())\n",
    "\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro\n",
    "\n",
    "# --- Check normality of the differences ---\n",
    "differences = df_wide[\"Port A\"] - df_wide[\"Port B\"]\n",
    "shapiro_stat, shapiro_p = shapiro(differences)\n",
    "\n",
    "print(\"\\nNormality Check (Shapiro-Wilk Test on paired differences):\")\n",
    "print(f\"Shapiro-Wilk statistic = {shapiro_stat:.3f}, p = {shapiro_p:.3g}\")\n",
    "\n",
    "# Decide which test to use based on p-value\n",
    "if shapiro_p > 0.05:\n",
    "    print(\"Differences are normally distributed (p > 0.05). Using paired t-test.\")\n",
    "    t_stat, p_val = ttest_rel(df_wide[\"Port A\"], df_wide[\"Port B\"])\n",
    "    print(f\"\\nPaired t-test result:\\nt = {t_stat:.3f}, p = {p_val:.3g}\")\n",
    "else:\n",
    "    print(\"Differences are NOT normally distributed (p ≤ 0.05). Using Wilcoxon signed-rank test.\")\n",
    "    try:\n",
    "        w_stat, p_val = wilcoxon(df_wide[\"Port A\"], df_wide[\"Port B\"])\n",
    "        print(f\"\\nWilcoxon signed-rank test result:\\nW = {w_stat:.3f}, p = {p_val:.3g}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Wilcoxon test failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c4637-00b6-41c0-9827-08e93b40f48a",
   "metadata": {},
   "source": [
    "#### Population impedances and equivalent RC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bae01-6d10-4bba-b421-267ab72551c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro\n",
    "\n",
    "# Configuration\n",
    "condition = 'saved_pkls' #'saved_stim_pkls' #' saved_pkls\n",
    "base_path = \"../PEDOT-DES\"\n",
    "\n",
    "\n",
    "def compute_series_R_C(magnitude, phase_deg, frequency=1000):\n",
    "    # Convert phase to radians\n",
    "    phase_rad = np.radians(phase_deg)\n",
    "\n",
    "    # Series resistance (real part)\n",
    "    R = magnitude * np.cos(phase_rad)\n",
    "\n",
    "    # Capacitive reactance (imag part)\n",
    "    X_C = magnitude * np.sin(phase_rad)\n",
    "\n",
    "    # Series capacitance\n",
    "    C = 1 / (2 * np.pi * frequency * abs(X_C)) if X_C != 0 else np.nan\n",
    "\n",
    "    return R, C\n",
    "\n",
    "\n",
    "all_rat_data = []\n",
    "\n",
    "# Step 1: Load all rhs\n",
    "good_channels = {1, 2, 3, 13, 18, 19, 29, 31, 22,10,21,11,28,4,27,26,24,7}  # small channels only\n",
    "rat_shared_channels_map = {}  # maps rat_id -> set of shared (port, channel)\n",
    "\n",
    "for rat_folder in os.listdir(base_path):\n",
    "    if not rat_folder.startswith(\"rat\"):\n",
    "        continue\n",
    "    \n",
    "    rat_path = os.path.join(base_path, rat_folder, \"rhs_recordings/%s\"%condition)\n",
    "    if not os.path.exists(rat_path):\n",
    "        continue\n",
    "\n",
    "    csv_files = [f for f in os.listdir(rat_path) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        continue\n",
    "\n",
    "    rat_frames = []\n",
    "    per_file_channels = []\n",
    "    if csv_files:\n",
    "        f = csv_files[0]\n",
    "        print(f)\n",
    "        print(rat_folder)\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(rat_path, f))\n",
    "            df = df.rename(columns={\n",
    "                \"port_name\": \"port\",\n",
    "                \"native_channel_name\": \"channel\",\n",
    "                \"electrode_impedance_magnitude\": \"impedance\",\n",
    "                \"electrode_impedance_phase\": \"phase\"\n",
    "            })\n",
    "            df[\"rat\"] = rat_folder\n",
    "            df = df[[\"rat\", \"port\", \"channel\", \"impedance\", \"phase\"]]\n",
    "            # Extract numeric channel ID\n",
    "            df[\"channel_num\"] = df[\"channel\"].str.extract(r\"(\\d+)$\").astype(int)\n",
    "            # Flag good channels\n",
    "            df[\"is_good_channel\"] = df[\"channel_num\"].isin(good_channels)\n",
    "            per_file_channels.append(set(zip(df[\"port\"], df[\"channel\"])))\n",
    "\n",
    "            # Compute R and C for each row and store the results\n",
    "            df[['equivalent_R', 'equivalent_C']] = df.apply(\n",
    "                lambda row: pd.Series(compute_series_R_C(row['impedance'], row['phase'])), axis=1)\n",
    "                \n",
    "            rat_frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {f}: {e}\")\n",
    "    \n",
    "    # Compute shared (port, channel) pairs\n",
    "    if rat_frames and per_file_channels:\n",
    "        shared = set.intersection(*per_file_channels)\n",
    "        rat_shared_channels_map[rat_folder] = shared\n",
    "        all_rat_data.append(pd.concat(rat_frames, ignore_index=True))\n",
    "\n",
    "# Combine and clean\n",
    "df_all = pd.concat(all_rat_data, ignore_index=True)\n",
    "df_all = df_all[~df_all[\"port\"].str.contains(\"Port C\", na=False)]\n",
    "\n",
    "# Report shared channel quality\n",
    "print(\"\\n--- SHARED CHANNEL QUALITY REPORT ---\")\n",
    "report = []\n",
    "for rat, shared in rat_shared_channels_map.items():\n",
    "    rat_df = df_all[df_all[\"rat\"] == rat]\n",
    "    for port in sorted(set(p for p, _ in shared if \"C\" not in p)):\n",
    "        port_shared = [(p, ch) for p, ch in shared if p == port]\n",
    "        good, bad = [], []\n",
    "        for p, ch in port_shared:\n",
    "            subset = rat_df[(rat_df[\"port\"] == p) & (rat_df[\"channel\"] == ch)]\n",
    "            if not subset.empty and subset[\"is_good_channel\"].any():\n",
    "                good.append(ch)\n",
    "            else:\n",
    "                bad.append(ch)\n",
    "        report.append({\n",
    "            \"rat\": rat, \"port\": port,\n",
    "            \"n_shared\": len(port_shared),\n",
    "            \"n_good\": len(good), \"good_channels\": \", \".join(good),\n",
    "            \"n_bad\": len(bad), \"bad_channels\": \", \".join(bad)\n",
    "        })\n",
    "\n",
    "# Filter good and valid channels\n",
    "df_filtered = df_all[df_all[\"is_good_channel\"] & (df_all[\"impedance\"] <= 100000)]\n",
    "\n",
    "# Compute per-rat averages\n",
    "metrics = [\"impedance\", \"phase\", \"equivalent_R\", \"equivalent_C\"]\n",
    "summary_df = df_filtered.groupby([\"rat\", \"port\"])[metrics].mean().unstack()\n",
    "summary_df = summary_df.dropna(subset=[(\"impedance\", \"Port A\"), (\"impedance\", \"Port B\")])\n",
    "\n",
    "# Paired statistics\n",
    "print(\"\\n--- PAIRED STATISTICS ---\")\n",
    "for metric in metrics:\n",
    "    a = summary_df[(metric, \"Port A\")]\n",
    "    b = summary_df[(metric, \"Port B\")]\n",
    "    diff = a - b\n",
    "    normal = shapiro(diff)[1] > 0.05\n",
    "    if normal:\n",
    "        t, p = ttest_rel(a, b)\n",
    "        print(f\"{metric.title()}: t = {t:.3f}, p = {p:.3g}\")\n",
    "    else:\n",
    "        w, p = wilcoxon(a, b)\n",
    "        print(f\"{metric.title()} (nonparametric): W = {w:.3f}, p = {p:.3g}\")\n",
    "\n",
    "# Plot per-rat paired means\n",
    "fig, axs = plt.subplots(1, len(metrics), figsize=(6 * len(metrics), 6))\n",
    "for i, metric in enumerate(metrics):\n",
    "    sns.boxplot(x=\"port\", y=metric, data=df_filtered, ax=axs[i], palette=\"Set2\")\n",
    "    sns.stripplot(x=\"port\", y=metric, data=df_filtered, ax=axs[i], color='black', alpha=0.3, jitter=True)\n",
    "    for rat_id, row in summary_df.iterrows():\n",
    "        axs[i].plot([\"Port A\", \"Port B\"],\n",
    "                    [row[(metric, \"Port A\")], row[(metric, \"Port B\")]],\n",
    "                    marker=\"o\", label=rat_id if i == 0 else \"\")\n",
    "    axs[i].set_title(f\"{metric.title()} per Rat\")\n",
    "    axs[i].set_ylabel(metric.title())\n",
    "    axs[i].grid(True)\n",
    "    if i == 0:\n",
    "        axs[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_path}/impedance_RC__pop_means_{condition}.svg\", dpi=300)\n",
    "\n",
    "# Boxplots for all values\n",
    "fig, axs = plt.subplots(1, len(metrics), figsize=(6 * len(metrics), 6))\n",
    "for i, metric in enumerate(metrics):\n",
    "    sns.boxplot(x=\"port\", y=metric, data=df_filtered, ax=axs[i], palette=\"Set2\")\n",
    "    sns.stripplot(x=\"port\", y=metric, data=df_filtered, ax=axs[i], color='black', alpha=0.3, jitter=True)\n",
    "    axs[i].set_title(f\"All Channels: {metric.title()}\")\n",
    "    axs[i].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_path}/impedance_RC_population_{condition}.svg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3a07c-d220-496c-ac64-cb2d48e402d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Folder path\n",
    "condition = 'evoked'  # 'baseline'\n",
    "experiments_path = \"../PEDOT-DES\"\n",
    "base_dir = f'{experiments_path}/{condition}'\n",
    "\n",
    "# Load all files\n",
    "summary_files = [f for f in os.listdir(base_dir) if f.endswith('spike_metrics_summary.csv')]\n",
    "records = []\n",
    "\n",
    "for file in summary_files:\n",
    "    rat_id = file.replace('_spike_metrics_summary.csv', '')\n",
    "    filepath = os.path.join(base_dir, file)\n",
    "    df = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    # Get average metrics\n",
    "    avg_metrics = df[df.index.str.startswith('avg_')]\n",
    "\n",
    "    for metric in avg_metrics.index:\n",
    "        try:\n",
    "            val_des = avg_metrics.loc[metric, 'Port A']\n",
    "            val_pss = avg_metrics.loc[metric, 'Port B']\n",
    "            records.append({\n",
    "                'rat_id': rat_id,\n",
    "                'metric': metric,\n",
    "                'PEDOT:DES': val_des,\n",
    "                'PEDOT:PSS': val_pss,\n",
    "                'delta': val_des - val_pss\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {metric} in {rat_id} due to error: {e}\")\n",
    "\n",
    "df_all = pd.DataFrame(records)\n",
    "\n",
    "# Find metrics that have a baseline counterpart\n",
    "baseline_pairs = []\n",
    "metrics = df_all['metric'].unique()\n",
    "for m in metrics:\n",
    "    if m.endswith('_signal'):\n",
    "        baseline = m.replace('_signal', '_baseline')\n",
    "        if baseline in metrics:\n",
    "            baseline_pairs.append((m, baseline))\n",
    "\n",
    "# Create plot with 3 subplots per metric if baseline exists\n",
    "n_rows = len(metrics)\n",
    "fig, axs = plt.subplots(n_rows, 3, figsize=(16, 4 * n_rows))\n",
    "fig.tight_layout(pad=5)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    df_m = df_all[df_all['metric'] == metric]\n",
    "    if df_m.shape[0] < 2:\n",
    "        continue\n",
    "\n",
    "    # Paired t-test between materials\n",
    "    t_stat, p_val = ttest_rel(df_m['PEDOT:PSS'], df_m['PEDOT:DES'])\n",
    "\n",
    "    # Line plot per rat\n",
    "    ax1 = axs[i, 0] if n_rows > 1 else axs[0]\n",
    "    for _, row in df_m.iterrows():\n",
    "        ax1.plot(['PEDOT:DES', 'PEDOT:PSS'], [row['PEDOT:DES'], row['PEDOT:PSS']], marker='o', label=row['rat_id'])\n",
    "    ax1.set_title(f\"{metric}\\nt={t_stat:.2f}, p={p_val:.4f}\")\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax1.grid(True)\n",
    "    if i == 0:\n",
    "        ax1.legend()\n",
    "\n",
    "    # Histogram of delta\n",
    "    ax2 = axs[i, 1] if n_rows > 1 else axs[1]\n",
    "    valid_deltas = df_m['delta'].dropna()\n",
    "    if not valid_deltas.empty:\n",
    "        ax2.hist(valid_deltas, bins=3)\n",
    "        ax2.axvline(0, linestyle='--', color='black')\n",
    "        ax2.set_title(f\"Δ (DES - PSS) for {metric}\")\n",
    "    else:\n",
    "        ax2.set_visible(False)\n",
    "    ax2.axvline(0, linestyle='--', color='black')\n",
    "    ax2.set_title(f\"Δ (DES - PSS) for {metric}\")\n",
    "    ax2.set_xlabel('Δ Value')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # If it's a \"signal\" metric with baseline, plot the relative increase vs baseline\n",
    "    if (metric, metric.replace('_signal', '_baseline')) in baseline_pairs:\n",
    "        signal_df = df_all[df_all['metric'] == metric].set_index('rat_id')\n",
    "        baseline_df = df_all[df_all['metric'] == metric.replace('_signal', '_baseline')].set_index('rat_id')\n",
    "\n",
    "        # Only include rats in both, and with no NaNs\n",
    "        merged_df = pd.merge(\n",
    "            signal_df[['PEDOT:DES', 'PEDOT:PSS']],\n",
    "            baseline_df[['PEDOT:DES', 'PEDOT:PSS']],\n",
    "            left_index=True, right_index=True,\n",
    "            suffixes=('_signal', '_baseline')\n",
    "        ).dropna()\n",
    "        \n",
    "        if not merged_df.empty:\n",
    "            des_inc = (merged_df['PEDOT:DES_signal'] - merged_df['PEDOT:DES_baseline'])\n",
    "            pss_inc = (merged_df['PEDOT:PSS_signal'] - merged_df['PEDOT:PSS_baseline'])\n",
    "        \n",
    "            t_stat_inc, p_val_inc = ttest_rel(pss_inc, des_inc)\n",
    "        \n",
    "            ax3 = axs[i, 2] if n_rows > 1 else axs[2]\n",
    "            for rat in merged_df.index:\n",
    "                ax3.plot(['PEDOT:DES', 'PEDOT:PSS'], [des_inc[rat], pss_inc[rat]], marker='o', label=rat)\n",
    "            ax3.axhline(0, linestyle='--', color='gray')\n",
    "            ax3.set_title(f\"Δ over Baseline ({metric})\\nt={t_stat_inc:.2f}, p={p_val_inc:.4f}\")\n",
    "            ax3.set_ylabel('Increase')\n",
    "            ax3.grid(True)\n",
    "            if i == 0:\n",
    "                ax3.legend()\n",
    "        \n",
    "            results.append({\n",
    "                'metric': f\"{metric} Δ\",\n",
    "                'n': merged_df.shape[0],\n",
    "                't_stat': t_stat_inc,\n",
    "                'p_val': p_val_inc\n",
    "            })\n",
    "        # Paired t-test onincrease\n",
    "        t_stat_inc, p_val_inc = ttest_rel(pss_inc, des_inc)\n",
    "\n",
    "        ax3 = axs[i, 2] if n_rows > 1 else axs[2]\n",
    "        for rat in merged_df.index:\n",
    "            ax3.plot(['PEDOT:DES', 'PEDOT:PSS'], [des_inc[rat], pss_inc[rat]], marker='o', label=rat)\n",
    "        ax3.axhline(0, linestyle='--', color='gray')\n",
    "        ax3.set_title(f\"Δ over Baseline ({metric})\\nt={t_stat_inc:.2f}, p={p_val_inc:.4f}\")\n",
    "        ax3.set_ylabel('Increase')\n",
    "        ax3.grid(True)\n",
    "        if i == 0:\n",
    "            ax3.legend()\n",
    "\n",
    "        results.append({\n",
    "            'metric': f\"{metric} Δ\",\n",
    "            'n': len(merged_df.index),\n",
    "            't_stat': t_stat_inc,\n",
    "            'p_val': p_val_inc\n",
    "        })\n",
    "\n",
    "    results.append({\n",
    "        'metric': metric,\n",
    "        'n': df_m.shape[0],\n",
    "        't_stat': t_stat,\n",
    "        'p_val': p_val\n",
    "    })\n",
    "\n",
    "# Summary table\n",
    "results_df = pd.DataFrame(results).sort_values(by='p_val')\n",
    "\n",
    "plt.savefig(f'{base_dir}/overview_metrics_{condition}.svg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732249d-de61-4a97-9be3-9cac8a49dc4e",
   "metadata": {},
   "source": [
    "#### Population analysis stimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63e7ab-59ac-4401-adbf-349ea1faeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# ------------------------------\n",
    "# STEP 1: Find all `overview_stim.csv` files recursively\n",
    "# ------------------------------\n",
    "experiments_path = \"../PEDOT-DES\"\n",
    "root_dir = f'{experiments_path}/stimulation/'  # Top-level folder\n",
    "pattern = os.path.join(root_dir, '**', 'overview_stim.csv')\n",
    "overview_files = glob.glob(pattern, recursive=True)\n",
    "\n",
    "print(f\"Found {len(overview_files)} overview files.\")\n",
    "\n",
    "all_data = []\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        if file == 'overview_stim.csv':\n",
    "            df = pd.read_csv(os.path.join(root, file))\n",
    "            df['nerve_id'] = os.path.basename(root)  # Add identifier\n",
    "            all_data.append(df)\n",
    "\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# ---- STEP 2: Clean and annotate ---- #\n",
    "# Convert current to numeric\n",
    "df_all['current_uA'] = df_all['current'].str.replace('uA', '', regex=False).astype(float)\n",
    "\n",
    "# Assign material per pulse number and port\n",
    "def get_material(row):\n",
    "    first_material = 'PEDOT:DES' if row['port'] == 'PortA' else 'PEDOT:PSS'\n",
    "    second_material = 'PEDOT:PSS' if first_material == 'PEDOT:DES' else 'PEDOT:DES'\n",
    "    return first_material if row['pulse'] <= 10 else second_material\n",
    "\n",
    "df_all['material'] = df_all.apply(get_material, axis=1)\n",
    "\n",
    "# Assign stimulation order\n",
    "df_all['order'] = df_all['port'].map({\n",
    "    'PortA': 'DES first',\n",
    "    'PortB': 'PSS first'\n",
    "})\n",
    "\n",
    "# Compute RMS delta from baseline, and MAX metrics\n",
    "df_all['delta_RMS'] = df_all['RMS'] - df_all['baseline_RMS']\n",
    "df_all['delta_MAX'] = df_all['MAX'] - df_all['baseline_MAX']\n",
    "\n",
    "# ---- STEP 3: Aggregate per nerve ---- #\n",
    "agg_data = (\n",
    "    df_all\n",
    "    .groupby(['current_uA', 'material', 'nerve_id'])\n",
    "    .agg(\n",
    "        delta_RMS_mean=('delta_RMS', 'mean'),\n",
    "        delta_RMS_std=('delta_RMS', 'std'),\n",
    "        pulse_count=('pulse', 'count'),\n",
    "        number_ch=('number_ch', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_data['delta_RMS_sem'] = agg_data['delta_RMS_std']  # Optional: normalize by pulse_count\n",
    "\n",
    "agg_data_MAX = (\n",
    "    df_all\n",
    "    .groupby(['current_uA', 'material', 'nerve_id'])\n",
    "    .agg(\n",
    "        delta_MAX_mean=('delta_MAX', 'mean'),\n",
    "        delta_MAX_std=('delta_MAX', 'std'),\n",
    "        pulse_count=('pulse', 'count'),\n",
    "        number_ch=('number_ch', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_data_MAX['delta_MAX_sem'] = agg_data_MAX['delta_MAX_std']\n",
    "\n",
    "agg_data_RMS = (\n",
    "    df_all\n",
    "    .groupby(['current_uA', 'material', 'nerve_id'])\n",
    "    .agg(\n",
    "        RMS_mean=('RMS', 'mean'),\n",
    "        RMS_std=('RMS', 'std'),\n",
    "        pulse_count=('pulse', 'count'),\n",
    "        number_ch=('number_ch', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_data_RMS['RMS_sem'] = agg_data_RMS['RMS_std']\n",
    "\n",
    "# ---- STEP 4: Plotting ---- #\n",
    "sns.set(style='whitegrid')\n",
    "fig, axs = plt.subplots(2,1 , figsize=(16, 10), sharex=True)\n",
    "\n",
    "# ----- ΔRMS plot ----- #\n",
    "for material, color in zip(['PEDOT:PSS', 'PEDOT:DES'], ['tab:blue', 'tab:orange']):\n",
    "    mat_data = agg_data[agg_data['material'] == material]\n",
    "\n",
    "    grouped = mat_data.groupby('current_uA').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'mean': np.average(x['delta_RMS_mean'], weights=x['pulse_count']),\n",
    "            'sem': np.sqrt(np.sum((x['delta_RMS_sem'] ** 2))) / len(x)\n",
    "        })\n",
    "    ).reset_index()\n",
    "\n",
    "    # Compute n per current\n",
    "    nerve_counts = (\n",
    "        mat_data.groupby('current_uA')['nerve_id']\n",
    "        .nunique()\n",
    "        .reset_index(name='n_nerves')\n",
    "    )\n",
    "    grouped = grouped.merge(nerve_counts, on='current_uA')\n",
    "\n",
    "    # Filter to valid currents\n",
    "    valid = grouped[grouped['n_nerves'] > 3]\n",
    "\n",
    "    axs[0].errorbar(valid['current_uA'], valid['mean'], yerr=valid['sem'],\n",
    "                    label=material, marker='o', color=color, capsize=4)\n",
    "\n",
    "    if material == 'PEDOT:PSS':\n",
    "        for _, row in valid.iterrows():\n",
    "            axs[0].text(row['current_uA'], row['mean'] + 20,\n",
    "                        f\"n={int(row['n_nerves'])}\", ha='center', color=color, fontsize=15)\n",
    "\n",
    "axs[0].set_title(\"Population ΔRMS (RMS - Baseline)\")\n",
    "axs[0].set_xlabel(\"Current (µA)\")\n",
    "axs[0].set_ylabel(\"ΔRMS\")\n",
    "axs[0].set_xlim([0, 32])\n",
    "axs[0].legend()\n",
    "\n",
    "# ----- RMS plot ----- #\n",
    "for material, color in zip(['PEDOT:PSS', 'PEDOT:DES'], ['tab:blue', 'tab:orange']):\n",
    "    mat_data = agg_data_RMS[agg_data_RMS['material'] == material]\n",
    "\n",
    "    grouped = mat_data.groupby('current_uA').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'mean': np.average(x['RMS_mean'], weights=x['pulse_count']),\n",
    "            'sem': np.sqrt(np.sum((x['RMS_sem'] ** 2))) / len(x)\n",
    "        })\n",
    "    ).reset_index()\n",
    "\n",
    "    nerve_counts = (\n",
    "        mat_data.groupby('current_uA')['nerve_id']\n",
    "        .nunique()\n",
    "        .reset_index(name='n_nerves')\n",
    "    )\n",
    "    grouped = grouped.merge(nerve_counts, on='current_uA')\n",
    "\n",
    "    valid = grouped[grouped['n_nerves'] > 3]\n",
    "\n",
    "    axs[1].errorbar(valid['current_uA'], valid['mean'], yerr=valid['sem'],\n",
    "                    label=material, marker='o', color=color, capsize=4)\n",
    "\n",
    "    if material == 'PEDOT:PSS':\n",
    "        for _, row in valid.iterrows():\n",
    "            axs[1].text(row['current_uA'], row['mean'] + 10,\n",
    "                        f\"n={int(row['n_nerves'])}\", ha='center', color=color, fontsize=15)\n",
    "\n",
    "axs[1].set_title(\"Population RMS\")\n",
    "axs[1].set_xlabel(\"Current (µA)\")\n",
    "axs[1].set_ylabel(\"RMS\")\n",
    "axs[1].set_xlim([0, 32])\n",
    "axs[1].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(f'{root_dir}/population_stimulation_with_counts_PortA_only_above3.svg', dpi=300)\n",
    "\n",
    "# ------------------------------\n",
    "# STEP 5: Plot population ΔRMS\n",
    "# ------------------------------\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = {'PEDOT:PSS': 'tab:blue', 'PEDOT:DES': 'tab:orange'}\n",
    "\n",
    "# Plot individual nerves\n",
    "for material in ['PEDOT:PSS', 'PEDOT:DES']:\n",
    "    mat_data = agg_data[agg_data['material'] == material]\n",
    "    for nerve_id in mat_data['nerve_id'].unique():\n",
    "        sub = mat_data[mat_data['nerve_id'] == nerve_id]\n",
    "        plt.plot(sub['current_uA'], sub['delta_RMS_mean'],\n",
    "                 color=colors[material], alpha=0.4, linestyle='-', marker='o', label=None)\n",
    "\n",
    "# Plot population mean ± SEM: Mean across nerves \n",
    "pop = agg_data.groupby(['current_uA', 'material']).agg(\n",
    "    delta_RMS_mean=('delta_RMS_mean', 'mean'),\n",
    "    delta_RMS_sem=('delta_RMS_mean', 'sem'),\n",
    "    nerve_count=('nerve_id', 'nunique')  # Count of unique nerve_ids\n",
    ").reset_index()\n",
    "\n",
    "for material in ['PEDOT:PSS', 'PEDOT:DES']:\n",
    "    data = pop[pop['material'] == material]\n",
    "    # Plot the data\n",
    "    for current in data['current_uA'].unique():\n",
    "        count = data[data['current_uA'] == current]['nerve_count'].values[0]\n",
    "        color = 'tab:blue' if material == 'PEDOT:PSS' else 'tab:orange'\n",
    "\n",
    "        plt.errorbar(data[data['current_uA'] == current]['current_uA'],\n",
    "                     data[data['current_uA'] == current]['delta_RMS_mean'],\n",
    "                     yerr=data[data['current_uA'] == current]['delta_RMS_sem'],\n",
    "                     color=color, linewidth=2.5, marker='o')\n",
    "\n",
    "        # Add text box with nerve count for each current\n",
    "        plt.text(data[data['current_uA'] == current]['current_uA'].values[0],\n",
    "                 data[data['current_uA'] == current]['delta_RMS_mean'].values[0],\n",
    "                 f'N={count}', color=color, fontsize=10,\n",
    "                 ha='center', va='bottom', bbox=dict(facecolor='white', edgecolor=color, boxstyle='round,pad=0.3'))\n",
    "\n",
    "plt.xlabel(\"Current (μA)\")\n",
    "plt.ylabel(\"ΔRMS = RMS - Baseline RMS\")\n",
    "plt.title(\"Population ΔRMS: PEDOT:PSS vs PEDOT:DES\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(f'{root_dir}/population_stimulation_allCurrents_individual_with_counts.svg', dpi=300)\n",
    "\n",
    "# ------------------------------\n",
    "# STEP 6: Paired statistical test at 10 μA and 30 μA\n",
    "# ------------------------------\n",
    "for target_uA in [10, 30]:\n",
    "    data = agg_data[agg_data['current_uA'] == target_uA]\n",
    "\n",
    "    # Pivot to align materials by nerve_id\n",
    "    pivot = data.pivot(index='nerve_id', columns='material', values='delta_RMS_mean').dropna()\n",
    "\n",
    "    print(f\"\\n--- Statistical Comparison at {target_uA} μA ---\")\n",
    "    if pivot.shape[0] >= 2:\n",
    "        differences = pivot['PEDOT:PSS'] - pivot['PEDOT:DES']\n",
    "        shapiro_stat, shapiro_p = shapiro(differences)\n",
    "\n",
    "        print(f\"  N = {pivot.shape[0]} nerves\")\n",
    "        print(f\"  Shapiro-Wilk normality test on differences:\")\n",
    "        print(f\"    W = {shapiro_stat:.3f}, p = {shapiro_p:.4g}\")\n",
    "\n",
    "        if shapiro_p > 0.05:\n",
    "            print(\"  → Differences are normally distributed (p > 0.05). Using paired t-test.\")\n",
    "            t_stat, p_val = ttest_rel(pivot['PEDOT:PSS'], pivot['PEDOT:DES'])\n",
    "            print(f\"  Paired t-test result:\\n    t = {t_stat:.3f}, p = {p_val:.4g}\")\n",
    "        else:\n",
    "            print(\"  → Differences are NOT normally distributed (p ≤ 0.05). Using Wilcoxon signed-rank test.\")\n",
    "            try:\n",
    "                w_stat, p_val = wilcoxon(pivot['PEDOT:PSS'], pivot['PEDOT:DES'])\n",
    "                print(f\"  Wilcoxon signed-rank test result:\\n    W = {w_stat:.3f}, p = {p_val:.4g}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"  Wilcoxon test failed: {e}\")\n",
    "        \n",
    "        print(\"  Interpretation:\")\n",
    "        print(\"    Positive difference → PEDOT:PSS > PEDOT:DES\")\n",
    "        print(\"    Negative difference → PEDOT:DES > PEDOT:PSS\")\n",
    "    else:\n",
    "        print(f\"  Not enough matched nerves with both materials at {target_uA} μA for statistical comparison.\")\n",
    "\n",
    "df_all.to_csv(f'{root_dir}/_all_nerves_overview_stim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04bba1-e200-4881-bc64-455894a6dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Threshold data (R1 to R4)\n",
    "data = {\n",
    "    'R1_RN': [6, 8.5],\n",
    "    'R1_LN': [8, 13],\n",
    "    'R2_RN': [6, 35],\n",
    "    'R2_LN': [32, 5],\n",
    "    'R3_RN': [8, 9],\n",
    "    'R3_LN': [3, 2],\n",
    "    'R4_RN': [7, 9],\n",
    "    'R4_LN': [8, 10]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot thresholds for each group (RN and LN)\n",
    "for group in df.columns:\n",
    "    plt.plot(df.index, df[group], marker='o', label=group)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Threshold (uA)')\n",
    "plt.title('Threshold Comparison: RN vs LN for Each Rat Group')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig(f'{root_dir}/stimulation_twitches_pairedComparison.svg', dpi=300)\n",
    "\n",
    "# Calculate the difference (DES - PSS) for each experiment (each column)\n",
    "increments = df.iloc[0] - df.iloc[1]  # DES - PSS for each experiment\n",
    "\n",
    "# Plot the increments (DES - PSS) for each experiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the increment for each experiment\n",
    "plt.bar(increments.index, increments.values, color='purple')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Experiments')\n",
    "plt.ylabel('Threshold Increment (DES - PSS) (uA)')\n",
    "plt.title('Threshold Increment (DES - PSS) for Each Experiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Perform paired t-tests between DES and PSS for each experiment\n",
    "results = {}\n",
    "for experiment in df.columns:\n",
    "    # Perform paired t-test for DES (first value) vs PSS (second value)\n",
    "    t_stat, p_val = ttest_rel(df[experiment][0], df[experiment][1])\n",
    "    \n",
    "    # Store results\n",
    "    results[experiment] = (t_stat, p_val)\n",
    "\n",
    "# Print the statistical results\n",
    "for experiment, (t_stat, p_val) in results.items():\n",
    "    print(f\"\\n{experiment}:\")\n",
    "    print(f\"  t = {t_stat:.3f}, p = {p_val:.3g}\")\n",
    "    if p_val < 0.05:\n",
    "        print(\"  The difference between DES and PSS is statistically significant.\")\n",
    "    else:\n",
    "        print(\"  The difference between DES and PSS is not statistically significant.\")\n",
    "\n",
    "plt.savefig(f'{root_dir}/stimulation_twitches.svg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02503d7c-f39a-4539-b10b-e7faf4d41b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
